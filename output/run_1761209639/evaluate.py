# -*- coding: utf-8 -*-
"""
evaluate.py â€” å­¦ç”Ÿç”»åƒ Â· ç¦»çº¿åéªŒè¯„ä¼°ï¼ˆæ‰«æ students_chunk_*.jsonlï¼‰
è¾“å‡ºï¼š
  1) report.json        â€”â€” å…¨é‡ç»Ÿè®¡ä¸å„æŒ‡æ ‡åˆ†ç»„ç»“æœï¼ˆå¹´çº§/æ€§åˆ«/æ‰¹æ¬¡ï¼‰
  2) report.md          â€”â€” äººç±»å¯è¯»çš„ç®€æŠ¥ï¼ˆçº¢é»„ç»¿ç¯ + å¯ç–‘æ ·æœ¬Top-Nï¼‰
  3) suspicious.csv     â€”â€” å¯ç–‘æ ·æœ¬æ¸…å•ï¼ˆä¾¿äºå›æº¯ï¼‰
  4) duplicates.csv     â€”â€” è¿‘é‡å¤/æ¨¡æ¿åŒ–æ ·æœ¬å¯¹ï¼ˆSimHash â‰¤ é˜ˆå€¼ï¼‰
  5) failures_samples.json â€”â€” è‹¥ä½ æŠŠå¤±è´¥æ ·æœ¬å†™åœ¨ run_dir/failures.jsonlï¼Œä¼šå¤åˆ¶ä¸€ä»½åšæ€»ç»“

è¯„ä»·ç»´åº¦ï¼ˆAâ€“Gï¼‰ï¼š
A å®Œæ•´æ€§ & ä¸¥æ ¼çº¦æŸï¼ˆå¿…å¡«é”®ã€å­¦æœ¯æ°´å¹³å››é€‰ä¸€ã€ä»£ç†åæ­£åˆ™ï¼‰
B ä½“è£ç»“æ„ï¼ˆä»·å€¼è§‚/åˆ›é€ åŠ›/å¿ƒç†å¥åº·ä¸ºâ€œå•æ®µâ€ï¼›æ˜¾å¼æ§½ä½å‘½ä¸­ï¼‰
C ä»·å€¼è§‚ä¸ƒç»´+ç­‰çº§è¯è¦†ç›–åº¦
D åˆ›é€ åŠ›å…«ç»´+é›·è¾¾æ€»ç»“ã€å¯è¡Œæ€§â†’æå‡ºæ–¹æ¡ˆä¸€è‡´æ€§
E å¿ƒç†å¥åº·ç»“æ„æ§½ä½ã€éè¯Šæ–­åŒ–
F è·¨ç»´ä¸€è‡´æ€§ï¼ˆå¹´çº§â†”å¹´é¾„ã€ä»·å€¼è§‚èº«å¿ƒå¥åº·â†”å¿ƒç†ä¸‰æŒ‡æ ‡ã€ç§‘ç›®ä¸äº¤å‰â€¦ï¼‰
G å¤šæ ·æ€§ä¸å»æ¨¡æ¿ï¼ˆSimHashè¿‘é‚»ç‡ã€é•¿åº¦/è¯æ±‡ç¦»æ•£åº¦ã€å¹´çº§Ã—æ€§åˆ«Ã—å­¦ç§‘ç°‡è¦†ç›–åº¦ï¼‰

ç”¨æ³•ï¼š
  python evaluate.py --input <dir or file> --outdir <dir> [--simhash_threshold 3] [--topn 50]
"""

import argparse, os, re, json, glob, math, hashlib, csv
from collections import Counter, defaultdict
from typing import List, Dict, Any, Tuple

STRICT_ALLOWED_STRINGS = {
    "é«˜ï¼šæˆç»©å…¨æ ¡æ’åå‰10%",
    "ä¸­ï¼šæˆç»©å…¨æ ¡æ’åå‰10%è‡³30%",
    "ä½ï¼šæˆç»©å…¨æ ¡æ’åå‰30%è‡³50%",
    "å·®ï¼šæˆç»©å…¨æ ¡æ’åå50%",
}

# â€”â€” ä»£ç†åæ­£åˆ™ï¼ˆä¿®æ­£ï¼šå…è®¸ 2~4 ä¸ªéŸ³èŠ‚ï¼ša1_b2 æˆ– a1_b2_c3 æˆ– a1_b2_c3_d2ï¼‰
AGENT_REGEX = re.compile(r"^[a-z]+[1-5]?(?:_[a-z]+[1-5]?){1,3}$")

PARA_FIELDS = ["ä»·å€¼è§‚","åˆ›é€ åŠ›","å¿ƒç†å¥åº·"]
VAL_7 = ["é“å¾·ä¿®å…»","èº«å¿ƒå¥åº·","æ³•æ²»æ„è¯†","ç¤¾ä¼šè´£ä»»","æ”¿æ²»è®¤åŒ","æ–‡åŒ–ç´ å…»","å®¶åº­è§‚å¿µ"]
CRE_8 = ["æµç•…æ€§","æ–°é¢–æ€§","çµæ´»æ€§","å¯è¡Œæ€§","é—®é¢˜å‘ç°","é—®é¢˜åˆ†æ","æå‡ºæ–¹æ¡ˆ","æ”¹å–„æ–¹æ¡ˆ"]
PSY_KWS = ["ç»¼åˆå¿ƒç†çŠ¶å†µ","å¹¸ç¦æŒ‡æ•°","æŠ‘éƒé£é™©","ç„¦è™‘é£é™©","ä¿¡æ¯ä¸è¶³æˆ–æœªè§æ˜¾è‘—ç—‡çŠ¶","èƒŒæ™¯","åº”å¯¹","æ”¯æŒ","å®¶åº­","åŒä¼´","è€å¸ˆ"]

GRADES_ORDER = ["ä¸€å¹´çº§","äºŒå¹´çº§","ä¸‰å¹´çº§","å››å¹´çº§","äº”å¹´çº§","å…­å¹´çº§","åˆä¸€","åˆäºŒ","åˆä¸‰","é«˜ä¸€","é«˜äºŒ","é«˜ä¸‰"]
GRADE_AGE = {
    "ä¸€å¹´çº§":(6,7),"äºŒå¹´çº§":(7,8),"ä¸‰å¹´çº§":(8,9),"å››å¹´çº§":(9,10),"äº”å¹´çº§":(10,11),"å…­å¹´çº§":(11,12),
    "åˆä¸€":(12,13),"åˆäºŒ":(13,14),"åˆä¸‰":(14,15),"é«˜ä¸€":(15,16),"é«˜äºŒ":(16,17),"é«˜ä¸‰":(17,18)
}

def load_jsonl(path: str) -> List[Dict[str,Any]]:
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line=line.strip()
            if not line: continue
            try:
                data.append(json.loads(line))
            except:
                pass
    return data

def scan_inputs(input_path: str) -> List[Dict[str,Any]]:
    if os.path.isdir(input_path):
        files = sorted(glob.glob(os.path.join(input_path, "students_chunk_*.jsonl")))
        out=[]
        for p in files: out.extend(load_jsonl(p))
        return out
    else:
        return load_jsonl(input_path)

# â€”â€” å·¥å…·ï¼šSimHash64
def _text_to_ngrams(t: str, n: int = 3) -> List[str]:
    t = re.sub(r"\s+", "", t)
    return [t[i:i+n] for i in range(max(0, len(t)-n+1))] if t else []

def simhash64(text: str) -> int:
    v = [0]*64
    for g in _text_to_ngrams(text, 3):
        h = int(hashlib.md5(g.encode("utf-8")).hexdigest(), 16)
        for i in range(64):
            v[i] += 1 if ((h >> i) & 1) else -1
    out = 0
    for i in range(64):
        if v[i] >= 0: out |= (1<<i)
    return out

def hamming(a: int, b: int) -> int:
    x=a^b; c=0
    while x: x&=x-1; c+=1
    return c

# â€”â€” ä½“è£åˆ¤æ–­
def is_single_paragraph(s: str) -> bool:
    if not isinstance(s, str): return False
    if re.search(r"(\n\s*\n)|(^\s*[-â€¢\d]+\.)", s): return False
    return True

def has_any(s: str, kws: List[str]) -> bool:
    return isinstance(s, str) and any(kw in s for kw in kws)

# â€”â€” Aï¼šå®Œæ•´æ€§ & ä¸¥æ ¼çº¦æŸ
def check_A(item: Dict[str,Any]) -> List[str]:
    req = ["id","å§“å","å¹´é¾„","æ“…é•¿ç§‘ç›®","è–„å¼±ç§‘ç›®","å¹´çº§","äººæ ¼","ç¤¾äº¤å…³ç³»","å­¦æœ¯æ°´å¹³","æ€§åˆ«","å‘å±•é˜¶æ®µ","ä»£ç†å","ä»·å€¼è§‚","åˆ›é€ åŠ›","å¿ƒç†å¥åº·"]
    issues=[]
    for k in req:
        v = item.get(k, None)
        if v is None or (isinstance(v,str) and not v.strip()) or (isinstance(v,(list,dict)) and len(v)==0):
            issues.append(f"A.missing:{k}")

    if item.get("å­¦æœ¯æ°´å¹³") not in STRICT_ALLOWED_STRINGS:
        issues.append("A.level_not_in_4")

    ag = str(item.get("ä»£ç†å",""))
    if not AGENT_REGEX.match(ag):
        issues.append("A.agent_regex_bad")

    # ç§‘ç›®ä¸äº¤å‰ & éç©ºæ•°ç»„
    good_subj=True
    major=item.get("æ“…é•¿ç§‘ç›®",[]); weak=item.get("è–„å¼±ç§‘ç›®",[])
    if not isinstance(major,list) or not isinstance(weak,list) or len(major)==0 or len(weak)==0:
        good_subj=False
    else:
        if set(major) & set(weak): good_subj=False
    if not good_subj:
        issues.append("A.subject_set_bad")

    return issues

# â€”â€” Bï¼šä½“è£ç»“æ„
def check_B(item: Dict[str,Any]) -> List[str]:
    issues=[]
    for f in PARA_FIELDS:
        if not is_single_paragraph(item.get(f,"")):
            issues.append(f"B.paragraph:{f}")
    # æ˜¾å¼æ§½ä½å‘½ä¸­ï¼ˆè½»é‡ï¼‰
    if not has_any(item.get("ä»·å€¼è§‚",""), VAL_7): issues.append("B.value_missing_dims")
    if not has_any(item.get("ä»·å€¼è§‚",""), ["é«˜","è¾ƒé«˜","ä¸­","ä¸­ä¸Š","è¾ƒä½","ä½"]): issues.append("B.value_missing_levels")
    if not has_any(item.get("åˆ›é€ åŠ›",""), CRE_8): issues.append("B.creativity_missing_dims")
    cr=item.get("åˆ›é€ åŠ›","")
    if ("é›·è¾¾" not in cr) and ("æ€»ç»“" not in cr):
        issues.append("B.creativity_no_radar_hint")
    if not has_any(item.get("å¿ƒç†å¥åº·",""), PSY_KWS): issues.append("B.psych_missing_slots")
    return issues

# â€”â€” Cï¼šä»·å€¼è§‚ä¸ƒç»´ & ç­‰çº§è¯è¦†ç›–ï¼ˆä¸¥æ ¼ä¸€ç‚¹ï¼šä¸ƒç»´éƒ½éœ€å‡ºç°ï¼‰
def check_C(item: Dict[str,Any]) -> List[str]:
    txt=item.get("ä»·å€¼è§‚","") or ""
    missing=[d for d in VAL_7 if d not in txt]
    issues=[]
    if missing:
        issues.append("C.values_missing:"+",".join(missing))
    # ç­‰çº§è¯ç²—æ£€
    if not re.search(r"(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)", txt):
        issues.append("C.values_no_levelword")
    return issues

# â€”â€” Dï¼šåˆ›é€ åŠ›å…«ç»´ + é›·è¾¾æ€»ç»“ + è§„åˆ™ï¼ˆå¯è¡Œæ€§ä½â†’æå‡ºæ–¹æ¡ˆâ‰¤ä¸­ï¼‰
def parse_level(s: str) -> int:
    # è¶Šå¤§è¶Šé«˜ï¼šä½(1) è¾ƒä½(2) ä¸­(3) ä¸­ä¸Š(4) è¾ƒé«˜(5) é«˜(6)
    # ç®€åŒ–æ˜ å°„ï¼ˆæ¨¡ç³ŠåŒ¹é…ï¼‰
    if "è¾ƒé«˜" in s: return 5
    if "ä¸­ä¸Š" in s: return 4
    if "é«˜" in s: return 6
    if "è¾ƒä½" in s: return 2
    if "ä½" in s: return 1
    if "ä¸­ç­‰" in s or "ä¸­" in s: return 3
    return 0

def check_D(item: Dict[str,Any]) -> List[str]:
    txt=item.get("åˆ›é€ åŠ›","") or ""
    issues=[]
    for d in CRE_8:
        if d not in txt:
            issues.append("D.creativity_missing:"+d)
    # â€œå…«ç»´ä¸å¾—å…¨éƒ¨ç›¸åŒâ€
    levels=[]
    for d in CRE_8:
        m=re.search(d+r"[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)", txt)
        levels.append(parse_level(m.group(1)) if m else 0)
    if levels and all(x==levels[0] for x in levels):
        issues.append("D.levels_all_equal")
    # å¯è¡Œæ€§ vs æå‡ºæ–¹æ¡ˆ
    lvl_feas=0; lvl_prop=6
    m1=re.search(r"å¯è¡Œæ€§[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)", txt);
    m2=re.search(r"æå‡ºæ–¹æ¡ˆ[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)", txt)
    if m1: lvl_feas=parse_level(m1.group(1))
    if m2: lvl_prop=parse_level(m2.group(1))
    if lvl_feas in (1,2) and lvl_prop>3:
        issues.append("D.rule_feas_low_prop_too_high")
    # é›·è¾¾æ€»ç»“æç¤º
    if ("é›·è¾¾" not in txt) and ("æ€»ç»“" not in txt):
        issues.append("D.no_radar_summary")
    return issues

# â€”â€” Eï¼šå¿ƒç†å¥åº·ç»“æ„ & éè¯Šæ–­åŒ–
def check_E(item: Dict[str,Any]) -> List[str]:
    txt=item.get("å¿ƒç†å¥åº·","") or ""
    issues=[]
    # å…³é”®æ§½ä½æ˜¯å¦å‡ºç°
    slots = {
        "ç»¼åˆå¿ƒç†çŠ¶å†µ": r"ç»¼åˆå¿ƒç†çŠ¶å†µ[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)",
        "å¹¸ç¦æŒ‡æ•°": r"å¹¸ç¦æŒ‡æ•°[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)",
        "æŠ‘éƒé£é™©": r"æŠ‘éƒé£é™©[^ã€‚ï¼›]*?(ä½|è½»åº¦|ä¸­åº¦|é‡åº¦)",
        "ç„¦è™‘é£é™©": r"ç„¦è™‘é£é™©[^ã€‚ï¼›]*?(ä½|è½»åº¦|ä¸­åº¦|é‡åº¦)",
    }
    for k,pat in slots.items():
        if not re.search(pat, txt):
            issues.append("E.psych_slot_missing:"+k)
    # éè¯Šæ–­åŒ–ï¼ˆç®€æ˜“é»‘è¯è¡¨ï¼‰
    blacklist = ["é‡åº¦æŠ‘éƒ","åŒç›¸","ä½é™¢","å¼ºè¿«ç—‡ä¸¥é‡","ç²¾ç¥åˆ†è£‚","å¤„æ–¹è¯æ»¥ç”¨"]
    for w in blacklist:
        if w in txt:
            issues.append("E.non_clinical_violation:"+w)
    return issues

# â€”â€” Fï¼šè·¨ç»´ä¸€è‡´æ€§
def check_F(item: Dict[str,Any]) -> List[str]:
    issues=[]
    # å¹´çº§ â†” å¹´é¾„ï¼ˆå…è®¸Â±1ï¼‰
    grade=item.get("å¹´çº§",""); age=item.get("å¹´é¾„",None)
    if grade in GRADE_AGE and isinstance(age,(int,float)):
        lo,hi=GRADE_AGE[grade]
        if not (lo-1 <= age <= hi+1):
            issues.append("F.grade_age_outlier")
    # ä»·å€¼è§‚â€œèº«å¿ƒå¥åº·â€è¾ƒé«˜/é«˜ â†’ å¿ƒç†ï¼šç»¼åˆå¿ƒç†çŠ¶å†µâ‰¥ä¸­, æŠ‘éƒ/ç„¦è™‘ â‰¤ ä¸­åº¦
    v=item.get("ä»·å€¼è§‚","") or ""
    if "èº«å¿ƒå¥åº·" in v and re.search(r"èº«å¿ƒå¥åº·[^ã€‚ï¼›]*?(è¾ƒé«˜|é«˜)", v):
        p=item.get("å¿ƒç†å¥åº·","") or ""
        ok=True
        m_z=re.search(r"ç»¼åˆå¿ƒç†çŠ¶å†µ[^ã€‚ï¼›]*?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­ç­‰|ä¸­|è¾ƒä½|ä½)", p)
        lvl = parse_level(m_z.group(1)) if m_z else 0
        if lvl and lvl<3: ok=False
        for risk in ["æŠ‘éƒé£é™©","ç„¦è™‘é£é™©"]:
            m_r=re.search(rf"{risk}[^ã€‚ï¼›]*?(ä½|è½»åº¦|ä¸­åº¦|é‡åº¦)", p)
            if m_r and ("é‡åº¦" in m_r.group(1)): ok=False
        if not ok:
            issues.append("F.value_psych_inconsistency")
    return issues

# â€”â€” Gï¼šå¤šæ ·æ€§ï¼ˆæ‰¹å†…è¿‘é‡å¤ç‡ï¼‰
def build_key_text(it: Dict[str,Any]) -> str:
    return "ï½œ".join([
        str(it.get("å¹´çº§","")), str(it.get("æ€§åˆ«","")),
        " ".join(it.get("äººæ ¼",[]) if isinstance(it.get("äººæ ¼"), list) else [str(it.get("äººæ ¼",""))]),
        str(it.get("ä»·å€¼è§‚","")), str(it.get("ç¤¾äº¤å…³ç³»","")),
        str(it.get("åˆ›é€ åŠ›","")), str(it.get("å¿ƒç†å¥åº·",""))
    ])

def measure_G(all_items: List[Dict[str,Any]], sim_th: int) -> Tuple[float, List[Tuple[int,int,int]]]:
    """è¿”å›ï¼šè¿‘é‡å¤å æ¯”, é‡å¤è¾¹( (i,j,hamming) ) åˆ—è¡¨ï¼ˆi<j æŒ‰ç´¢å¼•ï¼‰"""
    hashes=[]
    for it in all_items:
        hashes.append(simhash64(build_key_text(it)))
    dup_edges=[]
    n=len(hashes)
    if n<=1: return 0.0, dup_edges
    # ç®€å•O(n^2)ï¼ˆè‹¥å¾ˆå¤§å¯æ¢è¿‘é‚»ç´¢å¼•ï¼‰
    dup=0
    for i in range(n):
        for j in range(i+1,n):
            d=hamming(hashes[i],hashes[j])
            if d<=sim_th:
                dup+=1
                dup_edges.append((i,j,d))
    # å®šä¹‰å æ¯”ï¼šé‡å¤å¯¹æ•° / ç»„åˆæ•°
    ratio = dup / (n*(n-1)/2)
    return ratio, dup_edges

def eval_one(item: Dict[str,Any]) -> Dict[str,Any]:
    issues = []
    issues += check_A(item)
    issues += check_B(item)
    issues += check_C(item)
    issues += check_D(item)
    issues += check_E(item)
    issues += check_F(item)
    return {
        "id": item.get("id"),
        "å¹´çº§": item.get("å¹´çº§"),
        "æ€§åˆ«": item.get("æ€§åˆ«"),
        "å­¦æœ¯æ°´å¹³": item.get("å­¦æœ¯æ°´å¹³"),
        "ok": len(issues)==0,
        "issues": issues
    }

def bucket(x: Dict[str,Any], keys: List[str]) -> str:
    return " / ".join([str(x.get(k,"-")) for k in keys])

def to_md_badge(v: float, green: Tuple[float,float], yellow: Tuple[float,float], fmt="{:.1%}") -> str:
    """ç»¿åŒºé—´(green[0]~green[1])æ›´å¥½ï¼›å¦åˆ™é»„/çº¢"""
    s = fmt.format(v)
    if green[0] <= v <= green[1]: return f"ğŸŸ¢ {s}"
    if yellow[0] <= v <= yellow[1]: return f"ğŸŸ¡ {s}"
    return f"ğŸ”´ {s}"

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", required=True, help="è¾“å…¥ç›®å½•ï¼ˆåŒ…å« students_chunk_*.jsonlï¼‰æˆ–å•ä¸ª jsonl")
    ap.add_argument("--outdir", required=True, help="è¯„ä¼°è¾“å‡ºç›®å½•")
    ap.add_argument("--simhash_threshold", type=int, default=3)
    ap.add_argument("--topn", type=int, default=50, help="å¯ç–‘æ ·æœ¬Top-N")
    args = ap.parse_args()

    os.makedirs(args.outdir, exist_ok=True)
    items = scan_inputs(args.input)
    n = len(items)

    # å•æ¡è¯„ä¼°
    rows = [eval_one(it) for it in items]
    ok_cnt = sum(1 for r in rows if r["ok"])
    issue_flat = []
    for i,r in enumerate(rows):
        for iss in r["issues"]:
            issue_flat.append((i, iss))

    # è¿‘é‡å¤
    dup_ratio, dup_edges = measure_G(items, args.simhash_threshold)

    # åˆ†å¸ƒï¼šå¹´çº§/æ€§åˆ«/å­¦æœ¯æ°´å¹³
    dist_grade = Counter([it.get("å¹´çº§","-") for it in items])
    dist_gender = Counter([it.get("æ€§åˆ«","-") for it in items])
    dist_level = Counter([it.get("å­¦æœ¯æ°´å¹³","-") for it in items])

    # åˆ†ç»„é€šè¿‡ç‡
    def group_pass(keys: List[str]):
        g = defaultdict(lambda: {"total":0,"ok":0})
        for r in rows:
            k = " / ".join([str(r.get(x,"-")) for x in keys])
            g[k]["total"] += 1
            g[k]["ok"] += (1 if r["ok"] else 0)
        out=[]
        for k,v in g.items():
            rate = v["ok"]/v["total"] if v["total"] else 0.0
            out.append({"group":k,"total":v["total"],"ok":v["ok"],"pass_rate":rate})
        out.sort(key=lambda x: -x["total"])
        return out

    g_grade = group_pass(["å¹´çº§"])
    g_gender = group_pass(["æ€§åˆ«"])
    g_grade_gender = group_pass(["å¹´çº§","æ€§åˆ«"])

    # å¯ç–‘æ ·æœ¬Top-Nï¼ˆæŒ‰é—®é¢˜æ•°é™åºï¼‰
    scored = []
    for i,r in enumerate(rows):
        scored.append((i, len(r["issues"]), r["issues"]))
    scored.sort(key=lambda x: -x[1])
    topN = scored[:min(args.topn, len(scored))]

    # è¾“å‡ºï¼šJSON
    report = {
        "total": n,
        "ok_count": ok_cnt,
        "ok_rate": ok_cnt/n if n else 0.0,
        "dup_ratio": dup_ratio,
        "dup_edges_count": len(dup_edges),
        "dist": {
            "grade": dist_grade,
            "gender": dist_gender,
            "level": dist_level
        },
        "groups": {
            "by_grade": g_grade,
            "by_gender": g_gender,
            "by_grade_gender": g_grade_gender
        },
        "issues_overview": Counter([x[1] for x in issue_flat]),
        "topN_indices": [i for (i,_,_) in topN]
    }
    with open(os.path.join(args.outdir, "report.json"), "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # å¤åˆ¶å¤±è´¥æ ·æœ¬ï¼ˆè‹¥å­˜åœ¨ï¼‰
    fail_src = os.path.join(args.input, "failures.jsonl") if os.path.isdir(args.input) else None
    if fail_src and os.path.exists(fail_src):
        fails = load_jsonl(fail_src)
        with open(os.path.join(args.outdir, "failures_samples.json"), "w", encoding="utf-8") as f:
            json.dump(fails, f, ensure_ascii=False, indent=2)

    # suspicious.csv
    with open(os.path.join(args.outdir, "suspicious.csv"), "w", newline="", encoding="utf-8") as f:
        w=csv.writer(f)
        w.writerow(["index","id","grade","gender","issue_count","issues"])
        for i,cnt,iss in topN:
            it=items[i]
            w.writerow([i, it.get("id"), it.get("å¹´çº§"), it.get("æ€§åˆ«"), cnt, ";".join(iss)])

    # duplicates.csv
    with open(os.path.join(args.outdir, "duplicates.csv"), "w", newline="", encoding="utf-8") as f:
        w=csv.writer(f); w.writerow(["i","j","hamming","id_i","id_j","grade_i","grade_j"])
        for (i,j,d) in dup_edges:
            it_i=items[i]; it_j=items[j]
            w.writerow([i,j,d, it_i.get("id"), it_j.get("id"), it_i.get("å¹´çº§"), it_j.get("å¹´çº§")])

    # report.mdï¼ˆçº¢é»„ç»¿ç¯ï¼‰
    ok_rate = report["ok_rate"]
    # ç»éªŒé˜ˆï¼šok_rate ç»¿â‰¥0.9ï¼Œé»„0.8~0.9ï¼›dup_ratio ç»¿â‰¤0.01ï¼Œé»„0.01~0.03
    ok_badge  = to_md_badge(ok_rate, (0.90,1.01), (0.80,0.90))
    dup_badge = to_md_badge(dup_ratio, (0.0,0.01), (0.01,0.03))
    top_issue = report["issues_overview"].most_common(8)

    md = []
    md.append(f"# æ‰¹é‡åéªŒè¯„ä¼°ï¼ˆN={n}ï¼‰")
    md.append(f"- âœ… åˆæ ¼ç‡ï¼š{ok_badge}")
    md.append(f"- ğŸ§¬ è¿‘é‡å¤ç‡ï¼ˆSimHashâ‰¤{args.simhash_threshold}ï¼‰ï¼š{dup_badge}  ï¼ˆé‡å¤è¾¹ï¼š{len(dup_edges)}ï¼‰\n")
    md.append("## ä¸»è¦é—®é¢˜ Top-8")
    for k,v in top_issue:
        md.append(f"- {k} Ã— {v}")
    md.append("\n## åˆ†ç»„é€šè¿‡ç‡ï¼ˆå¹´çº§Ã—æ€§åˆ«ï¼‰Top-10")
    for row in report["groups"]["by_grade_gender"][:10]:
        md.append(f"- {row['group']}: {row['ok']}/{row['total']}ï¼ˆ{row['pass_rate']:.1%}ï¼‰")
    md.append("\n## å¯ç–‘æ ·æœ¬ Top-Nï¼ˆè¯¦è§ suspicious.csvï¼‰")
    for i,cnt,iss in topN[:10]:
        md.append(f"- idx {i} Â· issues={cnt}: {', '.join(iss[:6])}{'â€¦' if len(iss)>6 else ''}")

    with open(os.path.join(args.outdir, "report.md"), "w", encoding="utf-8") as f:
        f.write("\n".join(md))

    print(f"[OK] Wrote {args.outdir}/report.json, report.md, suspicious.csv, duplicates.csv")

if __name__ == "__main__":
    main()
                                                                                                                                                                                                       