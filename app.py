# -*- coding: utf-8 -*-
"""
app.py â€” å¤šæ™ºèƒ½ä½“ Â· å®æ—¶å¤šè½®åä½œ Â· å­¦ç”Ÿç”»åƒæ‰¹é‡ç”Ÿæˆï¼ˆå…¨éƒ¨å­—æ®µç”±æ™ºèƒ½ä½“ç»APIäº§å‡ºï¼‰
åœ¨çº¿å‰ç½®æ§åˆ¶ï¼šé…é¢åˆ†æ¡¶è°ƒåº¦ + è½»é‡è¿‡æ»¤ + è‡ªç›¸ä¼¼åº¦é˜ˆï¼ˆSimHashï¼‰
åç«¯ï¼šOrchestrator + 5 å†…å®¹Agent + Validatorï¼Œç™½æ¿é»‘æ¿æ¨¡å¼ + å¤šè½®åå•†
ä¾èµ–ï¼šstreamlit, requests
æ¥å£ï¼šä½¿ç”¨ AIECNU /v1/chat/completionsï¼ˆå·²ç¡¬ç¼–ç ï¼‰
!!! åŠŸèƒ½è¦ç‚¹ï¼š
1) æ— ç”Ÿæˆä¸Šé™ï¼šæŒ‰ 50 æ¡/ç‰‡ åˆ†ç‰‡ï¼Œæ”¯æŒä»»æ„å¤§ Nï¼›é¡¶æ â€œåˆ†ç‰‡è¿›åº¦â€ï¼Œä¸­éƒ¨â€œå½“å‰ç‰‡è¿›åº¦â€ï¼›
2) æ— åå•†è½®æ•°ä¸Šé™ï¼šè½®æ•° number_inputï¼ˆä¸è®¾ä¸Šé™ï¼‰ï¼›
3) æš‚åœ/ç»§ç»­ï¼šå¯éšæ—¶æš‚åœï¼›æš‚åœå³ä½œåºŸâ€œå½“å‰æ­£åœ¨æ„å»ºâ€çš„æ¡ç›®ï¼›ç»§ç»­è‡ªåŠ¨æ‰¾åˆ°æœ€åä¸€ç‰‡å¹¶ç»­å†™ï¼›
4) è‡ªåŠ¨è½ç›˜ï¼šç”Ÿæˆä¸€æ¡å°±å†™ä¸€æ¡åˆ°æœ¬åœ° `output/<run_id>/students_chunk_{i}.jsonl`ï¼›50æ¡ä¸ºä¸€ç‰‡ï¼Œè‡ªåŠ¨æ¢æ–°æ–‡ä»¶ï¼›
5) ä½“è£æ–°æ ‡å‡†ï¼šä»·å€¼è§‚/åˆ›é€ åŠ›/å¿ƒç†å¥åº· å¼ºåˆ¶â€œå•æ®µè¿ç»­è‡ªç„¶è¯­è¨€â€ï¼›ä¸€è‡´æ€§ä¸åˆè§„æ ¡éªŒï¼›
6) å­¦æœ¯æ°´å¹³ä¸¥æ ¼â€œå››é€‰ä¸€ï¼ˆå›ºå®šæ–‡æ¡ˆï¼‰â€ï¼›ä»£ç†åå…è®¸å¤šéŸ³èŠ‚ï¼ˆå§“1â€“2éŸ³èŠ‚ã€å1â€“3éŸ³èŠ‚ï¼Œæ¯èŠ‚æ‹¼éŸ³+1~5å£°è°ƒï¼Œç”¨ä¸‹åˆ’çº¿åˆ†éš”ï¼‰ï¼›
7) QuotaSchedulerï¼ˆå¹´çº§Ã—æ€§åˆ«Ã—ä¼˜åŠ¿å­¦ç§‘ç°‡ï¼‰å‰ç½®é‡‡æ ·ï¼›è½»é‡è¿‡æ»¤ï¼›SimHash å»åŒè´¨åŒ–ï¼›å¤±è´¥æ ·æœ¬è½ç›˜ï¼›
8) ğŸ–§ å®æ—¶äº¤äº’æ§åˆ¶å°ï¼ˆPrompt/Output/Issues å¯è§†åŒ–ï¼‰ï¼›
9) â˜… æ–°å¢ï¼šå­¦æœ¯æ°´å¹³åˆ†å¸ƒé”šå®š + è·¨ç»´åº¦â€œä¹è§‚åç½®â€æŠ‘åˆ¶ï¼ˆä»·å€¼è§‚/åˆ›é€ åŠ›/å¿ƒç†å¥åº·éšé”šè‡ªé€‚åº”ï¼Œå¹¶åœ¨è½»é‡è¿‡æ»¤ä¸­åšç¡¬é˜ˆæ ¡éªŒï¼‰ã€‚
"""

import json, re, random, math, os, glob, time, hashlib
from copy import deepcopy
from typing import Any, Dict, List, Tuple, Optional
import streamlit as st
import requests

# ================== ä½ çš„ APIï¼ˆæŒ‰è¦æ±‚ç¡¬ç¼–ç ï¼‰ ==================
AIECNU_API_KEY = "sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
AIECNU_BASE_URL = "http://xxxxxxxxxxx"
MODEL = "gpt-4.1"
HEADERS = {"Content-Type": "application/json", "Authorization": f"Bearer {AIECNU_API_KEY}"}

CHUNK_SIZE_DEFAULT = 50
MAX_RETRIES_PER_SLOT = 4
SIMHASH_BITS = 64
SIMHASH_HAMMING_THRESHOLD_DEFAULT = 3  # <=3 è§†ä¸ºè¿‡è¿‘ï¼Œé‡é‡‡

LEVEL_SET_STRICT = {
    "é«˜": "é«˜ï¼šæˆç»©å…¨æ ¡æ’åå‰10%",
    "ä¸­": "ä¸­ï¼šæˆç»©å…¨æ ¡æ’åå‰10%è‡³30%",
    "ä½": "ä½ï¼šæˆç»©å…¨æ ¡æ’åå‰30%è‡³50%",
    "å·®": "å·®ï¼šæˆç»©å…¨æ ¡æ’åå50%"
}
STRICT_ALLOWED_STRINGS = set(LEVEL_SET_STRICT.values())
LEVELS = [
    "é«˜ï¼šæˆç»©å…¨æ ¡æ’åå‰10%",
    "ä¸­ï¼šæˆç»©å…¨æ ¡æ’åå‰10%è‡³30%",
    "ä½ï¼šæˆç»©å…¨æ ¡æ’åå‰30%è‡³50%",
    "å·®ï¼šæˆç»©å…¨æ ¡æ’åå50%",
]
LEVEL_ALIAS = {
    "é«˜": LEVELS[0], "ä¸­": LEVELS[1], "ä½": LEVELS[2], "å·®": LEVELS[3],
    "high": LEVELS[0], "mid": LEVELS[1], "medium": LEVELS[1], "low": LEVELS[2], "poor": LEVELS[3]
}

# ä»£ç†åæ ¡éªŒæ­£åˆ™ï¼ˆæ”¯æŒ â‰¥2 éŸ³èŠ‚ï¼›å§“1-2èŠ‚ï¼Œå1-3èŠ‚ï¼›æ¯èŠ‚[a-z]++å£°è°ƒ1-5ï¼›å§“ä¸åä¹‹é—´ä¸€ä¸ªä¸‹åˆ’çº¿ï¼‰
AGENT_ID_REGEX = r"^(?:[a-z]+[1-5]){1,2}_(?:[a-z]+[1-5]){1,3}$"

GRADES = ["ä¸€å¹´çº§","äºŒå¹´çº§","ä¸‰å¹´çº§","å››å¹´çº§","äº”å¹´çº§","å…­å¹´çº§","åˆä¸€","åˆäºŒ","åˆä¸‰","é«˜ä¸€","é«˜äºŒ","é«˜ä¸‰"]
GENDERS = ["ç”·","å¥³"]
SUBJ_CLUSTERS = {
    "ç†ç§‘å‘": ["æ•°å­¦","ç‰©ç†","åŒ–å­¦","ä¿¡æ¯æŠ€æœ¯"],
    "æ–‡ç¤¾å‘": ["è¯­æ–‡","å†å²","æ”¿æ²»","åœ°ç†"],
    "è‰ºä½“å‘": ["ç¾æœ¯","éŸ³ä¹","ä½“è‚²"],
    "å¤–è¯­ç”Ÿç‰©å‘": ["è‹±è¯­","ç”Ÿç‰©"]
}

# ---- streamlit rerun å…¼å®¹å°è£… ----
def _st_rerun():
    try:
        st.rerun()
    except AttributeError:
        st.experimental_rerun()

# ================== å·¥å…·ï¼šè§£æå­¦æœ¯æ°´å¹³æ¯”ä¾‹ ==================
def _parse_level_mix(text: str) -> Dict[str, float]:
    """
    è§£æç”¨æˆ·è¾“å…¥çš„å­¦æœ¯æ°´å¹³é…æ¯”å­—ç¬¦ä¸²ï¼Œå¦‚ï¼š
      é«˜:0.25,ä¸­:0.25,ä½:0.25,å·®:0.25
      æˆ–è‹±æ–‡åˆ«åï¼šhigh:0.4,mid:0.3,low:0.2,poor:0.1
    è¿”å›ä¸¥æ ¼å››é€‰ä¸€æ–‡æ¡ˆçš„æ¯”ä¾‹å­—å…¸ï¼›éæ³•æˆ–ç¼ºå¤±è‡ªåŠ¨å‡åˆ†ã€‚
    """
    default = {LEVELS[0]:0.25, LEVELS[1]:0.25, LEVELS[2]:0.25, LEVELS[3]:0.25}
    if not text:
        return default
    try:
        kvs = [x.strip() for x in text.split(",") if x.strip()]
        acc = {}
        for kv in kvs:
            if ":" not in kv:
                continue
            k, v = [t.strip() for t in kv.split(":", 1)]
            k_std = LEVEL_ALIAS.get(k, k)
            if k_std not in STRICT_ALLOWED_STRINGS:
                continue
            acc[k_std] = float(v)
        if not acc: return default
        s = sum(acc.values())
        if s <= 0: return default
        for k in list(acc.keys()):
            acc[k] = acc[k] / s
        for l in LEVELS:
            acc.setdefault(l, 0.0)
        return acc
    except:
        return default

# ================== LLM è°ƒç”¨ä¸è§£æ ==================
def call_llm(messages: List[Dict[str, Any]], max_tokens=900, temperature=0.95) -> str:
    url = f"{AIECNU_BASE_URL.rstrip('/')}/chat/completions"
    payload = {"model": MODEL, "messages": messages, "max_tokens": max_tokens, "temperature": temperature}
    r = requests.post(url, headers=HEADERS, json=payload, timeout=120)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

def try_json(text: str) -> Dict[str, Any]:
    text = text.strip()
    try:
        return json.loads(text)
    except Exception:
        m = re.search(r"\{.*\}", text, flags=re.S)
        if m:
            try:
                return json.loads(m.group(0))
            except:
                return {}
        return {}

def non_empty(v: Any) -> bool:
    if v is None: return False
    if isinstance(v, str): return v.strip() != ""
    if isinstance(v, (list, dict)): return len(v) > 0
    return True

# ================== SimHashï¼ˆå»åŒè´¨åŒ–ï¼‰ ==================
def _text_to_ngrams(t: str, n: int = 3) -> List[str]:
    t = re.sub(r"\s+", "", t)
    return [t[i:i+n] for i in range(max(0, len(t)-n+1))] if t else []

def _simhash64(text: str) -> int:
    v = [0]*SIMHASH_BITS
    for g in _text_to_ngrams(text, 3):
        h = int(hashlib.md5(g.encode("utf-8")).hexdigest(), 16)
        for i in range(SIMHASH_BITS):
            v[i] += 1 if ((h >> i) & 1) else -1
    out = 0
    for i in range(SIMHASH_BITS):
        if v[i] >= 0:
            out |= (1 << i)
    return out

def _hamming(a: int, b: int) -> int:
    x = a ^ b
    cnt = 0
    while x:
        x &= x-1
        cnt += 1
    return cnt

class SimilarityGate:
    def __init__(self, threshold: int = SIMHASH_HAMMING_THRESHOLD_DEFAULT):
        self.threshold = threshold
        self.pool: List[int] = []

    def too_similar(self, text: str) -> bool:
        if not text: return False
        h = _simhash64(text)
        for prev in self.pool:
            if _hamming(h, prev) <= self.threshold:
                return True
        return False

    def accept(self, text: str):
        if not text: return
        self.pool.append(_simhash64(text))

# ================== è½»é‡è¿‡æ»¤ï¼ˆä½“è£/æ­£åˆ™/æ˜¾å¼å‘½ä¸­ + ä¹è§‚åç½®æŠ‘åˆ¶ï¼‰ ==================
AGENT_PARAGRAPH_FIELDS = ["ä»·å€¼è§‚","åˆ›é€ åŠ›","å¿ƒç†å¥åº·"]
VAL_DIMS7 = ["é“å¾·ä¿®å…»","èº«å¿ƒå¥åº·","æ³•æ²»æ„è¯†","ç¤¾ä¼šè´£ä»»","æ”¿æ²»è®¤åŒ","æ–‡åŒ–ç´ å…»","å®¶åº­è§‚å¿µ"]
LVL_WORDS = ["é«˜","è¾ƒé«˜","ä¸­ä¸Š","ä¸­","è¾ƒä½","ä½"]
CRE_DIMS8 = ["æµç•…æ€§","æ–°é¢–æ€§","çµæ´»æ€§","å¯è¡Œæ€§","é—®é¢˜å‘ç°","é—®é¢˜åˆ†æ","æå‡ºæ–¹æ¡ˆ","æ”¹å–„æ–¹æ¡ˆ"]
PSY_KEYS  = ["ç»¼åˆå¿ƒç†çŠ¶å†µ","å¹¸ç¦æŒ‡æ•°","æŠ‘éƒé£é™©","ç„¦è™‘é£é™©"]

def _is_single_paragraph(s: str) -> bool:
    if not isinstance(s, str): return False
    if re.search(r"(\n\s*\n)|(^\s*[-â€¢\d]+\.)", s): return False
    return True

def _has_any(s: str, kws: List[str]) -> bool:
    return any(kw in s for kw in kws)

def _count_levels(text: str) -> Dict[str, int]:
    cnt = {k:0 for k in LVL_WORDS}
    for k in LVL_WORDS:
        cnt[k] = len(re.findall(re.escape(k), text))
    return cnt

def _count_lowish(text: str) -> int:
    # ç»Ÿè®¡â€œä¸­/è¾ƒä½/ä½â€ï¼ˆä¸å«â€œä¸­ä¸Šâ€ï¼‰
    n_mid = len(re.findall(r"(?<!ä¸­)ä¸­(?!ä¸Š)", text))
    n_low = len(re.findall(r"è¾ƒä½|ä½", text))
    return n_mid + n_low

def _extract_dim_levels(text: str, dims: List[str]) -> Dict[str, str]:
    """
    è¿‘ä¼¼æŠ½å–æ¯ä¸ªç»´åº¦çš„ç­‰çº§è¯ï¼ˆæ­£åˆ™å¯å‘å¼ï¼‰ï¼Œç”¨äºå…«ç»´/ä¸ƒç»´ç²—æ ¡éªŒã€‚
    """
    res = {}
    for d in dims:
        # ç»´åº¦ååè‹¥å¹²å­—ç¬¦å†…çš„ç­‰çº§è¯
        m = re.search(d + r".{0,12}?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­(?!ä¸Š)|è¾ƒä½|ä½)", text)
        if m: res[d] = m.group(1)
    return res

def _light_filter(item: Dict[str, Any]) -> Tuple[bool, List[str]]:
    reasons = []
    # 1) å¿…å¡«é”®
    for k in ["å§“å","å¹´é¾„","æ€§åˆ«","å¹´çº§","äººæ ¼","æ“…é•¿ç§‘ç›®","è–„å¼±ç§‘ç›®","å­¦æœ¯æ°´å¹³","ä»·å€¼è§‚","åˆ›é€ åŠ›","å¿ƒç†å¥åº·","ä»£ç†å","å‘å±•é˜¶æ®µ","ç¤¾äº¤å…³ç³»"]:
        if k not in item or not non_empty(item[k]):
            reasons.append(f"ç¼ºå­—æ®µæˆ–ä¸ºç©ºï¼š{k}")

    # 2) å­¦æœ¯æ°´å¹³+ä»£ç†å+æ®µè½ä½“è£
    if item.get("å­¦æœ¯æ°´å¹³") not in STRICT_ALLOWED_STRINGS:
        reasons.append("å­¦æœ¯æ°´å¹³éå››é€‰ä¸€å›ºå®šæ–‡æ¡ˆ")
    if not re.match(AGENT_ID_REGEX, str(item.get("ä»£ç†å",""))):
        reasons.append("ä»£ç†åä¸åˆè§„ï¼ˆéœ€æ‹¼éŸ³åˆ†èŠ‚+å£°è°ƒæ•°å­—ï¼›å§“1-2èŠ‚ï¼Œå1-3èŠ‚ï¼‰")
    for f in AGENT_PARAGRAPH_FIELDS:
        if not _is_single_paragraph(item.get(f,"")):
            reasons.append(f"{f} éå•æ®µä½“è£")

    # 3) ä»·å€¼è§‚ï¼šä¸ƒç»´&ç­‰çº§è¯ + ä¹è§‚åç½®æŠ‘åˆ¶
    val = item.get("ä»·å€¼è§‚","")
    if not _has_any(val, VAL_DIMS7): reasons.append("ä»·å€¼è§‚æœªè§ä¸ƒç»´æ˜¾å¼åè¯ï¼ˆè‡³å°‘ç¼ºå¤§éƒ¨åˆ†ï¼‰")
    if not _has_any(val, LVL_WORDS): reasons.append("ä»·å€¼è§‚æœªè§ç­‰çº§è¯")
    # é”šå®šé©±åŠ¨çš„ä¸‹è°ƒè¦æ±‚
    target = item.get("_é‡‡æ ·çº¦æŸ",{}).get("ç›®æ ‡å­¦æœ¯æ°´å¹³") if isinstance(item.get("_é‡‡æ ·çº¦æŸ"), dict) else None
    lowish_need = 0
    if target in [LEVELS[1]]:      # ä¸­
        lowish_need = 1
    elif target in [LEVELS[2]]:    # ä½
        lowish_need = 2
    elif target in [LEVELS[3]]:    # å·®
        lowish_need = 3
    if lowish_need>0 and _count_lowish(val) < lowish_need:
        reasons.append(f"ä»·å€¼è§‚ç­‰çº§åˆ†å¸ƒè¿‡é«˜ï¼ˆé”š={target or 'æ— '}ï¼‰ï¼šéœ€è¦â‰¥{lowish_need}å¤„â€œä¸­/è¾ƒä½/ä½â€")

    # 4) åˆ›é€ åŠ›ï¼šå…«ç»´ + é›·è¾¾ + ä¹è§‚åç½®æŠ‘åˆ¶ + å†…éƒ¨ä¸€è‡´æ€§
    cre = item.get("åˆ›é€ åŠ›","")
    if not _has_any(cre, CRE_DIMS8): reasons.append("åˆ›é€ åŠ›æœªè§å…«ç»´æ˜¾å¼åè¯ï¼ˆè‡³å°‘ç¼ºå¤§éƒ¨åˆ†ï¼‰")
    if "é›·è¾¾" not in cre and "æ€»ç»“" not in cre: reasons.append("åˆ›é€ åŠ›æœªè§é›·è¾¾æ€»ç»“æç¤ºè¯")
    dimlv = _extract_dim_levels(cre, CRE_DIMS8)
    # è‡³å°‘ N ä¸ªç»´åº¦ä¸ºâ€œä¸­åŠä»¥ä¸‹â€ï¼ˆä¸å«â€œä¸­ä¸Šâ€ï¼‰
    lowish_cre = sum(1 for v in dimlv.values() if v in ["ä¸­","è¾ƒä½","ä½"])
    need = 0
    if target in [LEVELS[1]]:  # ä¸­
        need = 2
    elif target in [LEVELS[2]]:  # ä½
        need = 3
    elif target in [LEVELS[3]]:  # å·®
        need = 4
    if need>0 and lowish_cre < need:
        reasons.append(f"åˆ›é€ åŠ›å…«ç»´æ•´ä½“åé«˜ï¼ˆé”š={target or 'æ— '}ï¼‰ï¼šè¦æ±‚â‰¥{need}ä¸ªç»´åº¦ä¸ºâ€œä¸­åŠä»¥ä¸‹â€ï¼Œå½“å‰={lowish_cre}")
    # åŸæœ‰ä¸€è‡´æ€§ï¼šå¯è¡Œæ€§ä½â†’æå‡ºæ–¹æ¡ˆâ‰¤ä¸­
    if ("å¯è¡Œæ€§" in dimlv and dimlv.get("å¯è¡Œæ€§") in ["è¾ƒä½","ä½"]) and \
       ("æå‡ºæ–¹æ¡ˆ" in dimlv and dimlv.get("æå‡ºæ–¹æ¡ˆ") in ["é«˜","è¾ƒé«˜","ä¸­ä¸Š"]):
        reasons.append("åˆ›é€ åŠ›ä¸€è‡´æ€§ï¼šå¯è¡Œæ€§ä½ä½†â€˜æå‡ºæ–¹æ¡ˆâ€™é«˜")

    # 5) å¿ƒç†å¥åº·ï¼šå…³é”®æ§½ä½ + ä¹è§‚åç½®æŠ‘åˆ¶ï¼ˆå››æ§½ä½è‡³å°‘å‡ºç°ä¸­/è¾ƒä½/ä½ï¼›é£é™©ä¸èƒ½å…¨ä½äºâ€˜ä¸­â€™çš„åé¢ï¼‰
    psy = item.get("å¿ƒç†å¥åº·","")
    if not _has_any(psy, ["ç»¼åˆå¿ƒç†çŠ¶å†µ","å¹¸ç¦æŒ‡æ•°","æŠ‘éƒé£é™©","ç„¦è™‘é£é™©","ä¿¡æ¯ä¸è¶³æˆ–æœªè§æ˜¾è‘—ç—‡çŠ¶","èƒŒæ™¯","åº”å¯¹","æ”¯æŒ","å®¶åº­","åŒä¼´","è€å¸ˆ"]):
        reasons.append("å¿ƒç†å¥åº·æœªè§æ ¸å¿ƒæ§½ä½å…³é”®è¯")
    # ç²—æŠ½å››æ§½ä½ç­‰çº§
    psy_map = {}
    for k in PSY_KEYS:
        m = re.search(k + r".{0,12}?(é«˜|è¾ƒé«˜|ä¸­ä¸Š|ä¸­(?!ä¸Š)|è¾ƒä½|ä½|è½»åº¦|ä¸­åº¦|é‡åº¦|ä½é£é™©)", psy)
        if m: psy_map[k] = m.group(1)
    if target in [LEVELS[2], LEVELS[3]]:  # ä½/å·®
        # è¦æ±‚ï¼šç»¼åˆå¿ƒç†çŠ¶å†µ/å¹¸ç¦æŒ‡æ•° è‡³å°‘ä¸€ä¸ªä¸ºâ€œä¸­åŠä»¥ä¸‹â€ï¼›æŠ‘éƒ/ç„¦è™‘é£é™©ä¸å¾—éƒ½å†™æˆâ€œä½/ä½é£é™©â€
        cnt_mid_or_low = sum(1 for k in ["ç»¼åˆå¿ƒç†çŠ¶å†µ","å¹¸ç¦æŒ‡æ•°"] if psy_map.get(k) in ["ä¸­","è¾ƒä½","ä½"])
        if cnt_mid_or_low < 1:
            reasons.append("å¿ƒç†å¥åº·ä¸é”šä¸ç¬¦ï¼šç»¼åˆå¿ƒç†çŠ¶å†µ/å¹¸ç¦æŒ‡æ•°è‡³å°‘1å¤„éœ€â€œä¸­æˆ–è¾ƒä½/ä½â€")
        risk_lowish = 0
        for k in ["æŠ‘éƒé£é™©","ç„¦è™‘é£é™©"]:
            v = psy_map.get(k, "")
            if any(x in v for x in ["è½»åº¦","ä¸­åº¦"]):  # å…è®¸è½»/ä¸­
                risk_lowish += 1
        # è‹¥ä¸¤é¡¹éƒ½æ˜¾å¼â€œä½/ä½é£é™©â€ï¼Œåœ¨â€˜ä½/å·®â€™é”šä¸‹ä¸åˆç†
        if "æŠ‘éƒé£é™©" in psy_map and "ç„¦è™‘é£é™©" in psy_map:
            both_low = all(("ä½" in psy_map[k] or "ä½é£é™©" in psy_map[k]) for k in ["æŠ‘éƒé£é™©","ç„¦è™‘é£é™©"])
            if both_low:
                reasons.append("å¿ƒç†å¥åº·ä¸é”šä¸ç¬¦ï¼šæŠ‘éƒ/ç„¦è™‘é£é™©ä¸åº”åŒåŒä¸ºâ€˜ä½/ä½é£é™©â€™")
    return len(reasons) == 0, reasons

# ================== åä½œåŸºçŸ³ï¼šç™½æ¿ä¸è®¨è®ºï¼ˆæ”¯æŒIOæ—¥å¿—ï¼‰ ==================
REQUIRED_KEYS = ["id","å§“å","å¹´é¾„","æ“…é•¿ç§‘ç›®","è–„å¼±ç§‘ç›®","å¹´çº§","äººæ ¼","ç¤¾äº¤å…³ç³»",
                 "å­¦æœ¯æ°´å¹³","æ€§åˆ«","å‘å±•é˜¶æ®µ","ä»£ç†å","ä»·å€¼è§‚","åˆ›é€ åŠ›","å¿ƒç†å¥åº·"]

class Whiteboard:
    def __init__(self, sid: int, sampling_hint: Optional[Dict[str,Any]] = None):
        self.facts: Dict[str, Any] = {"id": sid}
        if sampling_hint:
            self.facts["_é‡‡æ ·çº¦æŸ"] = sampling_hint
        self.discussion: List[Dict[str, str]] = []

    def read(self) -> Dict[str, Any]: return deepcopy(self.facts)
    def write(self, patch: Dict[str, Any]):
        for k,v in patch.items():
            self.facts[k] = v

    def log(self, speaker: str, content: str):
        self.discussion.append({"speaker": speaker, "content": content})

    def serialize_for_agent(self) -> str:
        return json.dumps({"draft": self.facts, "discussion": self.discussion}, ensure_ascii=False)

# ================== åŸºç¡€æç¤ºè¯ï¼šç»Ÿä¸€åè®® ==================
AGENT_PREAMBLE = """ä½ æ˜¯ä¸€ä¸ªä¸å…¶ä»–æ™ºèƒ½ä½“åä½œçš„â€œå­¦ç”Ÿç”»åƒâ€ç”Ÿäº§æˆå‘˜ã€‚æˆ‘ä»¬ä½¿ç”¨â€œå…¬å…±ç™½æ¿â€å…±äº«è‰ç¨¿ä¸è®¨è®ºã€‚
è§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ï¼š
- æ‰€æœ‰è¾“å‡ºå¿…é¡»æ˜¯ **åˆæ³• JSON å¯¹è±¡**ï¼Œä¸”åªåŒ…å«ä½ è´Ÿè´£çš„é”®ã€‚
- ä¸å¾—å¼•ç”¨æ¨¡æ¿å¥å¼ï¼›ç”¨è‡ªç„¶ä¸­æ–‡ï¼›é¿å…ç©ºè¯å¥—è¯ï¼›é¿å…ä¸ç™½æ¿è‰ç¨¿è‡ªç›¸çŸ›ç›¾ã€‚
- è‹¥è¢«è¦æ±‚ä¿®è®¢ï¼Œåªæ”¹ä½ è´Ÿè´£çš„é”®ï¼›ä¸ç•™ç©ºï¼›ä¿è¯ä¸å…¶å®ƒå­—æ®µé€»è¾‘ä¸€è‡´ã€‚
- å§“åç­‰ä¸­æ–‡ï¼›æ•°å­—ä¸ç™¾åˆ†ä½è¯·ç”¨ä¸­æ–‡è¯­å¢ƒä¹¦å†™ï¼ˆå¦‚â€œå‰10%â€ï¼‰ã€‚
- ä¸è¦è¾“å‡ºä»»ä½•å¤šä½™è¯´æ˜æ–‡å­—ã€‚åªè¾“å‡º JSONã€‚
- å¦‚ç™½æ¿ä¸­å­˜åœ¨â€œ_é‡‡æ ·çº¦æŸâ€ï¼Œè¯·ä¸¥æ ¼éµå¾ªå…¶ä¸­çš„â€œå¹´çº§â€â€œæ€§åˆ«â€â€œä¼˜åŠ¿å­¦ç§‘åå‘â€â€œç›®æ ‡å­¦æœ¯æ°´å¹³â€ç­‰è¦æ±‚ï¼›è‹¥å‘ç”Ÿå†²çªï¼Œä»¥é‡‡æ ·çº¦æŸä¸ºå‡†å¹¶ä¿æŒæ•´ä½“ä¸€è‡´æ€§ã€‚
"""

RESP_FIELDS = {
    "å­¦ç±ä¸å‘å±•é˜¶æ®µ": ["å§“å","å¹´é¾„","æ€§åˆ«","å¹´çº§","å‘å±•é˜¶æ®µ","ä»£ç†å"],
    "å­¦ä¸šç”»åƒ": ["æ“…é•¿ç§‘ç›®","è–„å¼±ç§‘ç›®","å­¦æœ¯æ°´å¹³"],
    "äººæ ¼ä¸ä»·å€¼è§‚": ["äººæ ¼","ä»·å€¼è§‚"],
    "ç¤¾äº¤ä¸åˆ›é€ åŠ›": ["ç¤¾äº¤å…³ç³»","åˆ›é€ åŠ›"],
    "èº«å¿ƒå¥åº·": ["å¿ƒç†å¥åº·"]
}

def _pack_prompt(instruction: str, wb: Whiteboard) -> str:
    return f"ã€INSTRUCTIONã€‘\n{instruction}\n\nã€WHITEBOARDã€‘\n{wb.serialize_for_agent()}"

# ================== å„ Agentï¼ˆå«è‡ªé€‚åº”é”šæŒ‡å¼• + IOæ—¥å¿—ï¼‰ ==================
def agent_scholar(wb: Whiteboard, seed: str, mode: str="propose") -> Dict[str,Any]:
    sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
    hint = ""
    if sampling:
        hint = f"\né‡‡æ ·çº¦æŸï¼ˆéµå¾ªï¼‰ï¼šå¹´çº§={sampling.get('å¹´çº§','æœªæŒ‡å®š')}ï¼Œæ€§åˆ«={sampling.get('æ€§åˆ«','æœªæŒ‡å®š')}ï¼Œç›®æ ‡å­¦æœ¯æ°´å¹³={sampling.get('ç›®æ ‡å­¦æœ¯æ°´å¹³','æ— ')}ã€‚"
    instruction = f"""{AGENT_PREAMBLE}{hint}
ä½ è´Ÿè´£é”®ï¼š{RESP_FIELDS["å­¦ç±ä¸å‘å±•é˜¶æ®µ"]}
ä»»åŠ¡æ¨¡å¼ï¼š{mode}
å¤šæ ·æ€§ç§å­ï¼š{seed}

ç”Ÿæˆä¸çº¦æŸï¼ˆå¿…é¡»ï¼‰ï¼š
- å¹´é¾„ 6~18ï¼›å¹´é¾„**å¿…é¡»æ˜¯ä¸€ä¸ªé˜¿æ‹‰ä¼¯æ•°å­—**ï¼Œå¹´çº§ä¸å¹´é¾„åŒ¹é…ï¼ˆå…è®¸Â±1å¹´è·³çº§/ç•™çº§ä½†éœ€ä¸å…¶ä»–æ®µè½ä¸€è‡´ï¼‰ï¼›
- å‘å±•é˜¶æ®µå¯¹è±¡å¿…é¡»å«ä¸‰é”®ï¼šçš®äºšæ°è®¤çŸ¥å‘å±•é˜¶æ®µã€åŸƒé‡Œå…‹æ£®å¿ƒç†ç¤¾ä¼šå‘å±•é˜¶æ®µã€ç§‘å°”ä¼¯æ ¼é“å¾·å‘å±•é˜¶æ®µï¼›
- ä»£ç†åæ ¼å¼ï¼ˆ**å¤šéŸ³èŠ‚æ”¯æŒ**ï¼‰ï¼šå§“ 1~2 éŸ³èŠ‚ã€å 1~3 éŸ³èŠ‚ï¼›æ¯ä¸ªéŸ³èŠ‚ä¸ºâ€œæ‹¼éŸ³å°å†™+å£°è°ƒæ•°å­—(1-5)â€ï¼›å§“ä¸åä¹‹é—´ç”¨ä¸‹åˆ’çº¿ï¼›ç¤ºä¾‹ï¼š
  - å•å§“å•åï¼šzhang1_shuang3
  - å•å§“åŒåï¼šli1_huan4ying1
  - å¤å§“åŒåï¼šou3yang2_ming2hao3
ä»…è¾“å‡º JSONã€‚
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\nè¯·ä»…è¾“å‡ºä½ è´Ÿè´£çš„ JSONã€‚"}
    ]
    wb.log("å­¦ç±ä¸å‘å±•é˜¶æ®µâ†’prompt", _pack_prompt(instruction, wb))
    out = call_llm(messages, max_tokens=700, temperature=0.98)
    wb.log("å­¦ç±ä¸å‘å±•é˜¶æ®µâ†output", out)
    return try_json(out)

def agent_academic(wb: Whiteboard, seed: str, mode: str="propose") -> Dict[str,Any]:
    sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
    prefer = sampling.get("ä¼˜åŠ¿å­¦ç§‘åå‘")
    prefer_str = f"è¯·ä¼˜å…ˆä½¿â€œæ“…é•¿ç§‘ç›®â€è¦†ç›–è¯¥ç°‡ä¸­çš„è‡³å°‘1é—¨ï¼š{prefer}ã€‚" if prefer else ""
    target_level = sampling.get("ç›®æ ‡å­¦æœ¯æ°´å¹³")
    target_line = f"ã€å¼ºçº¦æŸã€‘æœ¬æ ·æœ¬çš„â€œå­¦æœ¯æ°´å¹³â€å¿…é¡»ä¸¥æ ¼ç­‰äºï¼š{target_level}ï¼›ä¸å¾—æ”¹ä¸ºå…¶å®ƒæ¡£ä½ã€‚" if target_level else "ï¼ˆæ— ç›®æ ‡é”šï¼‰"

    instruction = f"""{AGENT_PREAMBLE}
ä½ è´Ÿè´£é”®ï¼š{RESP_FIELDS["å­¦ä¸šç”»åƒ"]}
ä»»åŠ¡æ¨¡å¼ï¼š{mode}
å¤šæ ·æ€§ç§å­ï¼š{seed}

è¦æ±‚ï¼ˆå¿…é¡»ï¼‰ï¼š
- â€œæ“…é•¿ç§‘ç›®â€ä¸â€œè–„å¼±ç§‘ç›®â€å‡ä¸ºéç©ºæ•°ç»„ï¼Œä¸”ä¸¤è€…**é›†åˆä¸ç›¸äº¤**ï¼›
- â€œå­¦æœ¯æ°´å¹³â€**ä¸¥æ ¼å››é€‰ä¸€ï¼Œä¸”å­—ç¬¦ä¸²å¿…é¡»å®Œå…¨ç­‰äºä»¥ä¸‹ä¹‹ä¸€**ï¼š
  1) "é«˜ï¼šæˆç»©å…¨æ ¡æ’åå‰10%"
  2) "ä¸­ï¼šæˆç»©å…¨æ ¡æ’åå‰10%è‡³30%"
  3) "ä½ï¼šæˆç»©å…¨æ ¡æ’åå‰30%è‡³50%"
  4) "å·®ï¼šæˆç»©å…¨æ ¡æ’åå50%"
- {prefer_str}
- {target_line}

ä»…è¾“å‡º JSONï¼ˆåªå«â€œæ“…é•¿ç§‘ç›®â€â€œè–„å¼±ç§‘ç›®â€â€œå­¦æœ¯æ°´å¹³â€ä¸‰ä¸ªé”®ï¼‰ã€‚
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\nè¯·ä»…è¾“å‡ºä½ è´Ÿè´£çš„ JSONã€‚"}
    ]
    wb.log("å­¦ä¸šç”»åƒâ†’prompt", _pack_prompt(instruction, wb))
    out = call_llm(messages, max_tokens=600, temperature=0.9)
    wb.log("å­¦ä¸šç”»åƒâ†output", out)
    data = try_json(out)

    # å…œåº•å½’ä¸€ + å¼ºåˆ¶å¯¹é½ç›®æ ‡é”šï¼ˆå¦‚å­˜åœ¨ï¼‰
    if isinstance(data, dict):
        lvl = data.get("å­¦æœ¯æ°´å¹³")
        if isinstance(lvl, str):
            for k, v in LEVEL_SET_STRICT.items():
                if lvl.startswith(k) or k in lvl:
                    data["å­¦æœ¯æ°´å¹³"] = v; break
        if target_level and data.get("å­¦æœ¯æ°´å¹³") != target_level:
            data["å­¦æœ¯æ°´å¹³"] = target_level
    return data if isinstance(data, dict) else {}

def agent_values(wb: Whiteboard, seed: str, mode: str="propose") -> Dict[str,Any]:
    sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
    target = sampling.get("ç›®æ ‡å­¦æœ¯æ°´å¹³")
    adapt = ""
    if target in [LEVELS[1], LEVELS[2], LEVELS[3]]:
        adapt = ("- ã€éšå­¦æœ¯é”šè‡ªé€‚åº”ã€‘å½“ç›®æ ‡ä¸ºâ€œä¸­/ä½/å·®â€æ—¶ï¼Œä¸ƒç»´ä¸­çš„ç­‰çº§è¯åº”å‘ˆ**ä¸å‡è¡¡ä½†åŒ…å«è‹¥å¹²â€œä¸­/è¾ƒä½/ä½â€**ï¼Œ"
                 "é¿å…å…¨é«˜/è¾ƒé«˜ï¼›å¹¶ç»™å‡ºä¸ä¹‹åŒ¹é…çš„èƒŒæ™¯åŒ–æ ¹æ®ï¼ˆå¦‚å­¦ä¹ ä¹ æƒ¯/åé¦ˆ/ç¤¾å›¢è¡¨ç°ç­‰ï¼‰ã€‚")
    instruction = f"""{AGENT_PREAMBLE}
ä½ è´Ÿè´£é”®ï¼š{RESP_FIELDS["äººæ ¼ä¸ä»·å€¼è§‚"]}
ä»»åŠ¡æ¨¡å¼ï¼š{mode}
å¤šæ ·æ€§ç§å­ï¼š{seed}

è¾“å‡ºä½“è£ï¼ˆå¼ºçº¦æŸï¼‰ï¼šå•æ®µè¿ç»­è‡ªç„¶è¯­è¨€ï¼›**è¦†ç›–ä¸ƒç»´å¹¶æœ‰ç­‰çº§è¯**ï¼ˆé“å¾·ä¿®å…»ã€èº«å¿ƒå¥åº·ã€æ³•æ²»æ„è¯†ã€ç¤¾ä¼šè´£ä»»ã€æ”¿æ²»è®¤åŒã€æ–‡åŒ–ç´ å…»ã€å®¶åº­è§‚å¿µï¼‰ï¼›ç»™å‡ºèƒŒæ™¯åŒ–ä¾æ®ã€‚
{adapt}
ä»…è¾“å‡º JSONã€‚
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\nè¯·ä»…è¾“å‡ºä½ è´Ÿè´£çš„ JSONã€‚"}
    ]
    wb.log("äººæ ¼ä¸ä»·å€¼è§‚â†’prompt", _pack_prompt(instruction, wb))
    out = call_llm(messages, max_tokens=900, temperature=1.0)
    wb.log("äººæ ¼ä¸ä»·å€¼è§‚â†output", out)
    return try_json(out)

def agent_social_creative(wb: Whiteboard, seed: str, mode: str="propose") -> Dict[str,Any]:
    sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
    target = sampling.get("ç›®æ ‡å­¦æœ¯æ°´å¹³")
    adapt = ""
    if target in [LEVELS[1], LEVELS[2], LEVELS[3]]:
        adapt = ("- ã€éšå­¦æœ¯é”šè‡ªé€‚åº”ã€‘å½“ç›®æ ‡ä¸ºâ€œä¸­/ä½/å·®â€æ—¶ï¼Œå…«ç»´ç­‰çº§åˆ†å¸ƒ**å¿…é¡»åŒ…å«è‹¥å¹²â€œä¸­/è¾ƒä½/ä½â€**ï¼ˆè‡³å°‘2/3/4ä¸ªç»´åº¦ï¼‰ï¼Œ"
                 "å¹¶ä¿æŒå¯è¡Œæ€§ä¸æå‡ºæ–¹æ¡ˆçš„ä¸€è‡´æ€§ï¼›æœ«å°¾é›·è¾¾æ€»ç»“æ®æ­¤æ¦‚æ‹¬å¼ºå¼±ã€‚")
    instruction = f"""{AGENT_PREAMBLE}
ä½ è´Ÿè´£é”®ï¼š{RESP_FIELDS["ç¤¾äº¤ä¸åˆ›é€ åŠ›"]}
ä»»åŠ¡æ¨¡å¼ï¼š{mode}
å¤šæ ·æ€§ç§å­ï¼š{seed}

ç¤¾äº¤å…³ç³»ï¼šå•æ®µï¼ˆ160~260å­—ï¼‰ï¼ŒèƒŒæ™¯â†’å…³é”®äº‹ä»¶â†’å½±å“ï¼›ä¸å¾—æ¢è¡Œ/æ¡åˆ—ã€‚
åˆ›é€ åŠ›ï¼šå•æ®µï¼›**å…«ç»´ï¼ˆæµç•…æ€§/æ–°é¢–æ€§/çµæ´»æ€§/å¯è¡Œæ€§/é—®é¢˜å‘ç°/é—®é¢˜åˆ†æ/æå‡ºæ–¹æ¡ˆ/æ”¹å–„æ–¹æ¡ˆ å„æœ‰ç­‰çº§è¯ï¼‰+ é›·è¾¾æ€»ç»“**ï¼›å…«ç»´ä¸å¾—å…¨åŒæ¡£ï¼›è‹¥â€œå¯è¡Œæ€§â€è¾ƒä½/ä½ï¼Œåˆ™â€œæå‡ºæ–¹æ¡ˆâ€ä¸é«˜äºä¸­ç­‰ã€‚
{adapt}
ä»…è¾“å‡º JSONã€‚
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\nè¯·ä»…è¾“å‡ºä½ è´Ÿè´£çš„ JSONã€‚"}
    ]
    wb.log("ç¤¾äº¤ä¸åˆ›é€ åŠ›â†’prompt", _pack_prompt(instruction, wb))
    out = call_llm(messages, max_tokens=1100, temperature=1.02)
    wb.log("ç¤¾äº¤ä¸åˆ›é€ åŠ›â†output", out)
    return try_json(out)

def agent_health(wb: Whiteboard, seed: str, mode: str="propose") -> Dict[str,Any]:
    sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
    target = sampling.get("ç›®æ ‡å­¦æœ¯æ°´å¹³")
    adapt = ""
    if target in [LEVELS[2], LEVELS[3]]:
        adapt = ("- ã€éšå­¦æœ¯é”šè‡ªé€‚åº”ã€‘å½“ç›®æ ‡ä¸ºâ€œä½/å·®â€æ—¶ï¼Œâ€œç»¼åˆå¿ƒç†çŠ¶å†µ/å¹¸ç¦æŒ‡æ•°â€ä¸­è‡³å°‘ä¸€é¡¹å®œä¸ºâ€œä¸­æˆ–è¾ƒä½/ä½â€ï¼›"
                 "æŠ‘éƒ/ç„¦è™‘é£é™©é¿å…åŒåŒâ€˜ä½â€™ï¼›ä»é¡»ä¿æŒ**éè¯Šæ–­åŒ–**ä¸â€œå¯æ”¯æŒã€å¯æ”¹å–„â€çš„æ•™è‚²è¯­å¢ƒã€‚")
    instruction = f"""{AGENT_PREAMBLE}
ä½ è´Ÿè´£é”®ï¼š{RESP_FIELDS["èº«å¿ƒå¥åº·"]}
ä»»åŠ¡æ¨¡å¼ï¼š{mode}
å¤šæ ·æ€§ç§å­ï¼š{seed}

å¿ƒç†å¥åº·ï¼šå•æ®µï¼›ä¾æ¬¡å†…åµŒ æ¦‚è¿°â†’æ€§æ ¼ç‰¹å¾(â‰¥2)â†’ç»¼åˆå¿ƒç†çŠ¶å†µ/å¹¸ç¦æŒ‡æ•°/æŠ‘éƒé£é™©/ç„¦è™‘é£é™©â†’å¿ƒç†ç–¾ç—…ï¼ˆå¦‚æ— å†™â€œä¿¡æ¯ä¸è¶³æˆ–æœªè§æ˜¾è‘—ç—‡çŠ¶â€ï¼‰â†’èƒŒæ™¯æ•…äº‹â†’æ”¯æ’‘ä¸åº”å¯¹ï¼›éè¯Šæ–­åŒ–ï¼›ä¸ä»·å€¼è§‚â€œèº«å¿ƒå¥åº·â€ä¸€è‡´ã€‚
{adapt}
ä»…è¾“å‡º JSONã€‚
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\nè¯·ä»…è¾“å‡ºä½ è´Ÿè´£çš„ JSONã€‚"}
    ]
    wb.log("èº«å¿ƒå¥åº·â†’prompt", _pack_prompt(instruction, wb))
    out = call_llm(messages, max_tokens=1100, temperature=0.96)
    wb.log("èº«å¿ƒå¥åº·â†output", out)
    return try_json(out)

def agent_validator(wb: Whiteboard, seed: str) -> Dict[str, Any]:
    instruction = f"""ä½ æ˜¯â€œValidatorâ€æ™ºèƒ½ä½“ã€‚è¯·ä¸¥æ ¼å®¡æ ¡å¹¶ç»™å‡º**ç»“æ„åŒ–ä¿®è®¢ä»»åŠ¡**ã€‚
{AGENT_PREAMBLE}
ä½ åªè¾“å‡º JSONï¼Œé”®ä¸º issues ä¸ final_readyã€‚ä¸è¦è¾“å‡ºå¤šä½™æ–‡å­—ã€‚
"""
    rules = f"""è§„åˆ™å‚è€ƒï¼ˆå¿…é¡»ï¼‰ï¼š
- R1 å¹´é¾„â†”å¹´çº§å¸¸æ¨¡ï¼š6-7ä¸€å¹´çº§ï¼›7-8äºŒï¼›8-9ä¸‰ï¼›9-10å››ï¼›10-11äº”ï¼›11-12å…­ï¼›12-13åˆä¸€ï¼›13-14åˆäºŒï¼›14-15åˆä¸‰ï¼›15-16é«˜ä¸€ï¼›16-17é«˜äºŒï¼›17-18é«˜ä¸‰ï¼ˆå…è®¸Â±1å¹´å†…åå·®ï¼‰ã€‚
- R2 å‘å±•é˜¶æ®µä¸å¹´é¾„ï¼š~12å²ä»¥ä¸‹å¤šä¸ºâ€œå…·ä½“è¿ç®—â€ï¼›~12å²ä»¥ä¸Šâ€œå½¢å¼è¿ç®—â€ã€‚åŸƒé‡Œå…‹æ£®ï¼š6-12å‹¤å¥‹vsè‡ªå‘ï¼›12-18èº«ä»½vsè§’è‰²æ··ä¹±ï¼›ç§‘å°”ä¼¯æ ¼ï¼š~10å‰ä¹ ä¿—ã€~10-15ä¹ ä¿—ã€â‰¥15å¯å‘åä¹ ä¿—è¿‡æ¸¡ã€‚
- R3 ç§‘ç›®é›†åˆä¸äº¤å‰ã€ä¸”å‡éç©ºã€‚
- R4 åˆ›é€ åŠ›å…«ç»´ç­‰çº§éœ€æœ‰èµ·ä¼ï¼Œé¿å…å…¨éƒ¨ç›¸åŒï¼›è‹¥â€œå¯è¡Œæ€§â€è¾ƒä½/ä½ï¼Œåˆ™â€œæå‡ºæ–¹æ¡ˆâ€ä¸é«˜äºä¸­ç­‰ã€‚
- R5 ä»·å€¼è§‚ç§¯æç¨³å¥æ—¶ï¼Œå¿ƒç†æ®µè½ä¸å¾—å‡ºç°ä¸¥é‡åŠŸèƒ½å—æŸæˆ–é‡åº¦ä¸´åºŠæœ¯è¯­ã€‚
- R6 ä»£ç†åæ­£åˆ™ï¼š^[a-z]+[1-5]?_[a-z]+[1-5]?$
- R7 æ‰€æœ‰å¿…å¡«é”®ä¸å¯ä¸ºç©ºï¼šid, å§“å, å¹´é¾„, æ“…é•¿ç§‘ç›®, è–„å¼±ç§‘ç›®, å¹´çº§, äººæ ¼, ç¤¾äº¤å…³ç³», å­¦æœ¯æ°´å¹³, æ€§åˆ«, å‘å±•é˜¶æ®µ, ä»£ç†å, ä»·å€¼è§‚, åˆ›é€ åŠ›, å¿ƒç†å¥åº·ã€‚
- R8 ä»·å€¼è§‚ï¼šå¿…é¡»è¦†ç›–ä¸ƒç»´ï¼ˆé“å¾·ä¿®å…»/èº«å¿ƒå¥åº·/æ³•æ²»æ„è¯†/ç¤¾ä¼šè´£ä»»/æ”¿æ²»è®¤åŒ/æ–‡åŒ–ç´ å…»/å®¶åº­è§‚å¿µï¼‰ï¼Œæ¯ç»´å«å¯è¯†åˆ«ç­‰çº§è¯ï¼›å…è®¸è‡ªç„¶é¡ºåºä¸è‡ªç”±å¥æ³•ï¼Œä½†éœ€å¯å®šä½ã€‚
- R9 åˆ›é€ åŠ›ï¼šå¿…é¡»å« æ¦‚è¿° + å…«ç»´ï¼ˆæµç•…æ€§/æ–°é¢–æ€§/çµæ´»æ€§/å¯è¡Œæ€§/é—®é¢˜å‘ç°/é—®é¢˜åˆ†æ/æå‡ºæ–¹æ¡ˆ/æ”¹å–„æ–¹æ¡ˆï¼Œé€ç»´æœ‰ç­‰çº§è¯ä¸ç®€çŸ­ä¾æ®ï¼‰+ é›·è¾¾æ€»ç»“ã€‚
- R10 å¿ƒç†å¥åº·ï¼šå¿…é¡»å« æ¦‚è¿° + æ€§æ ¼ç‰¹å¾(â‰¥2ç‚¹) + ä¸‰ç»´åº¦ï¼ˆç»¼åˆå¿ƒç†çŠ¶å†µ/å¹¸ç¦æŒ‡æ•°/æŠ‘éƒé£é™©ä¸ç„¦è™‘é£é™©ï¼‰ + å¿ƒç†ç–¾ç—…ï¼ˆè‹¥æ— å†™â€œä¿¡æ¯ä¸è¶³æˆ–æœªè§æ˜¾è‘—ç—‡çŠ¶â€ï¼Œè‹¥æœ‰å†™â€œè¯Šæ–­æˆ–å€¾å‘/åŠŸèƒ½å½±å“/å½“å‰æ”¯æŒä¸å¤„ç†â€ï¼‰ + èƒŒæ™¯æ•…äº‹ + æ”¯æ’‘ä¸åº”å¯¹ã€‚
- R11 ä¸€è‡´æ€§ï¼š
    Â· è‹¥ä»·å€¼è§‚â€œèº«å¿ƒå¥åº·â€ä¸ºâ€œè¾ƒé«˜/é«˜â€ï¼Œåˆ™å¿ƒç†â€œç»¼åˆå¿ƒç†çŠ¶å†µâ€â‰¥ä¸­ç­‰ï¼Œä¸”â€œæŠ‘éƒ/ç„¦è™‘é£é™©â€â‰¤ä¸­åº¦ï¼›å¦‚æ¶‰åŠç–¾ç—…ï¼Œéœ€â€œå·²ç®¡ç†ã€åŠŸèƒ½åŸºæœ¬ç¨³å®šâ€ï¼›
    Â· å®¶åº­è§‚å¿µè¾ƒé«˜ä¸ç‹¬ç«‹æ€§ä¸å†²çªï¼Œåº”å‘ˆç°â€œäº’åŠ¨æ”¯æŒã€è¾¹ç•Œæ¸…æ™°â€ï¼›
    Â· ä»·å€¼è§‚/ç¤¾äº¤/å­¦ä¸šå™äº‹äº’ç›¸æ”¯æ’‘ï¼Œä¸å¾—çŸ›ç›¾ï¼ˆå¦‚ç¤¾äº¤å›é¿ vs é¢‘ç¹åä½œï¼‰ã€‚
- R12 éè¯Šæ–­åŒ–è¯­è¨€ï¼šé¿å…â€œé‡åº¦æŠ‘éƒ/åŒç›¸/ç”¨è¯/ä½é™¢â€ç­‰é‡ä¸´åºŠè¡¨è¿°ï¼›å…è®¸â€œå€¾å‘/è½»åº¦/èŠ‚ç‚¹æ€§/é˜¶æ®µæ€§/å¯ç®¡ç†/å»ºè®®å’¨è¯¢â€ç­‰ã€‚
- R13 å¯è¯»æ€§ä¸é¿å…æ¨¡æ¿ï¼šå†…å®¹åº”è‡ªç„¶è¿è´¯ï¼Œæ‹’ç»æµæ°´è´¦ä¸æœºæ¢°å¤è¿°ï¼›è‹¥â€œç­‰çº§è¯â€ç¼ºå¤±æˆ–ç»´åº¦ç¼ºå¤±ï¼Œæå‡ºä¿®è®¢ã€‚
- R14 æ®µè½åŒ–ä½“è£ï¼šä»·å€¼è§‚/åˆ›é€ åŠ›/å¿ƒç†å¥åº·å¿…é¡»ä¸º**å•æ®µè¿ç»­è‡ªç„¶è¯­è¨€**ï¼Œä¸å¾—ä½¿ç”¨åˆ—è¡¨ã€ç¼–å·ã€é¡¹ç›®ç¬¦å·æˆ–å¤šæ®µæ¢è¡Œï¼›å¦‚æ£€æµ‹åˆ°â€œ\\n\\nâ€ã€â€œ1.â€ã€â€œ- â€ã€â€œâ€¢ â€ç­‰æ¡åˆ—ç—•è¿¹ï¼Œåº”è¦æ±‚å¯¹åº”Owneré‡å†™ä¸ºå•æ®µã€‚
- R15 è‹¥â€œå­¦æœ¯æ°´å¹³â€ä¸åœ¨å…è®¸é›†åˆï¼Œå¿…é¡»è¦æ±‚â€œå­¦ä¸šç”»åƒâ€Owneré‡å†™å¹¶æ›¿æ¢ä¸º**ä¸¥æ ¼å››é€‰ä¸€å›ºå®šæ–‡æ¡ˆ**ã€‚
è¾“å‡ºï¼šissues: [{{code, desc, owner, fields, hint}}], final_ready: bool
"""
    messages = [
        {"role":"developer","content":instruction},
        {"role":"user","content":f"å…¬å…±ç™½æ¿ï¼š\n{wb.serialize_for_agent()}\n{rules}\nè¯·è¾“å‡º JSONã€‚"}
    ]
    wb.log("Validatorâ†’prompt", _pack_prompt(instruction + "\n\n" + rules, wb))
    out = call_llm(messages, max_tokens=1100, temperature=0.2)
    wb.log("Validatorâ†output", out)
    data = try_json(out)
    # æœ¬åœ°å…œåº•ï¼ˆå­¦æœ¯æ°´å¹³ã€ä»£ç†åã€ä¸ç›®æ ‡é”šä¸€è‡´æ€§ï¼‰
    try:
        lvl = wb.read().get("å­¦æœ¯æ°´å¹³", "")
        if lvl not in STRICT_ALLOWED_STRINGS:
            issues = data.get("issues", []) if isinstance(data, dict) else []
            issues.append({
                "code":"R14",
                "desc":"å­¦æœ¯æ°´å¹³æœªä¸¥æ ¼åŒ¹é…å…è®¸é›†åˆã€‚",
                "owner":"å­¦ä¸šç”»åƒ",
                "fields":["å­¦æœ¯æ°´å¹³"],
                "hint":"æ›¿æ¢ä¸ºå››é€‰ä¸€å›ºå®šæ–‡æ¡ˆï¼š'é«˜ï¼šæˆç»©å…¨æ ¡æ’åå‰10%' / 'ä¸­ï¼šæˆç»©å…¨æ ¡æ’åå‰10%è‡³30%' / 'ä½ï¼šæˆç»©å…¨æ ¡æ’åå‰30%è‡³50%' / 'å·®ï¼šæˆç»©å…¨æ ¡æ’åå50%'"})
            data = {"issues": issues, "final_ready": False}
        agent_id = wb.read().get("ä»£ç†å", "")
        if not re.match(AGENT_ID_REGEX, str(agent_id)):
            issues = data.get("issues", []) if isinstance(data, dict) else []
            issues.append({
                "code":"R6",
                "desc":"ä»£ç†åä¸åˆè§„ï¼ˆåº”ä¸ºå¤šéŸ³èŠ‚æ‹¼éŸ³+å£°è°ƒæ•°å­—ï¼Œå§“1-2èŠ‚ï¼Œå1-3èŠ‚ï¼Œå§“ä¸åç”¨ä¸‹åˆ’çº¿åˆ†éš”ï¼‰ã€‚",
                "owner":"å­¦ç±ä¸å‘å±•é˜¶æ®µ",
                "fields":["ä»£ç†å"],
                "hint":"ç¤ºä¾‹ï¼šzhang1_shuang3 / li1_huan4ying1 / ou3yang2_ming2hao3"})
            data = {"issues": issues, "final_ready": False}
        sampling = wb.read().get("_é‡‡æ ·çº¦æŸ", {})
        target_level = sampling.get("ç›®æ ‡å­¦æœ¯æ°´å¹³")
        if target_level and lvl != target_level:
            issues = data.get("issues", []) if isinstance(data, dict) else []
            issues.append({
                "code":"R14-anchored",
                "desc":f"ä¸é‡‡æ ·ç›®æ ‡å­¦æœ¯æ°´å¹³ä¸ä¸€è‡´ï¼ˆæœŸæœ›ï¼š{target_level}ï¼Œå®é™…ï¼š{lvl}ï¼‰ã€‚",
                "owner":"å­¦ä¸šç”»åƒ",
                "fields":["å­¦æœ¯æ°´å¹³"],
                "hint":f"å°†â€œå­¦æœ¯æ°´å¹³â€æ”¹ä¸ºç›®æ ‡æ¡£ä½ï¼š{target_level}ï¼›å…¶ä½™å­—æ®µåšè½»å¾®ä¸€è‡´æ€§ä¿®è®¢ã€‚"})
            data = {"issues": issues, "final_ready": False}
    except Exception:
        pass
    try:
        wb.log("Validator(issues)", json.dumps(data.get("issues", []), ensure_ascii=False))
    except Exception:
        wb.log("Validator(issues)", "[]")
    return data if data else {"issues":[{"code":"SYS","desc":"è§£æå¤±è´¥ï¼Œè¯·å„Agentè‡ªæ£€å¹¶é‡è¿°å…¶è´Ÿè´£å­—æ®µã€‚","owner":"å­¦ç±ä¸å‘å±•é˜¶æ®µ","fields":["å§“å"],"hint":"é‡æ–°å®Œæ•´ç»™å‡ºã€‚"}],"final_ready":False}

# ================== Orchestrator ==================
class Orchestrator:
    def __init__(self, max_rounds:int=3):
        self.max_rounds = max_rounds
        self.used_names: set = set()

    def _seed(self) -> str:
        return f"SEED-{random.randrange(10**16,10**17-1)}"

    def _merge_and_log(self, wb: Whiteboard, patch: Dict[str, Any], agent_name: str):
        wb.write(patch)
        wb.log(agent_name+"(åˆå¹¶)", json.dumps(patch, ensure_ascii=False))

    def run_one(self, sid: int, sampling_hint: Optional[Dict[str,Any]] = None) -> Tuple[Dict[str, Any], List[Dict[str,str]]]:
        wb = Whiteboard(sid, sampling_hint=sampling_hint)
        wb.log("System", f"ä»¥ä¸‹å§“åå·²è¢«ä½¿ç”¨ï¼ˆè¯·é¿å…é‡å¤ï¼‰ï¼š{list(self.used_names)}")
        seed = self._seed()

        self._merge_and_log(wb, agent_scholar(wb, seed, "propose"), "å­¦ç±ä¸å‘å±•é˜¶æ®µ")
        name_now = wb.read().get("å§“å")
        if name_now: self.used_names.add(name_now)

        self._merge_and_log(wb, agent_academic(wb, seed, "propose"), "å­¦ä¸šç”»åƒ")
        self._merge_and_log(wb, agent_values(wb, seed, "propose"), "äººæ ¼ä¸ä»·å€¼è§‚")
        self._merge_and_log(wb, agent_social_creative(wb, seed, "propose"), "ç¤¾äº¤ä¸åˆ›é€ åŠ›")
        self._merge_and_log(wb, agent_health(wb, seed, "propose"), "èº«å¿ƒå¥åº·")

        for r in range(1, self.max_rounds+1):
            v = agent_validator(wb, self._seed())
            issues = v.get("issues", [])
            final_ready = bool(v.get("final_ready", False))
            if final_ready and not issues:
                wb.log("Orchestrator", f"ç¬¬{r}è½®ï¼šValidatoré€šè¿‡âœ… æ— éœ€ç»§ç»­ä¿®è®¢ã€‚")
                break
            wb.log("Orchestrator", f"ç¬¬{r}è½®ï¼šæ”¶åˆ° {len(issues)} ä¸ªä¿®è®¢ä»»åŠ¡ã€‚")
            owners = {
                "å­¦ç±ä¸å‘å±•é˜¶æ®µ": agent_scholar,
                "å­¦ä¸šç”»åƒ": agent_academic,
                "äººæ ¼ä¸ä»·å€¼è§‚": agent_values,
                "ç¤¾äº¤ä¸åˆ›é€ åŠ›": agent_social_creative,
                "èº«å¿ƒå¥åº·": agent_health
            }
            wb.log("Validator(issues)", json.dumps(issues, ensure_ascii=False))
            touched = set()
            for it in issues:
                owner = it.get("owner")
                if owner in owners and owner not in touched:
                    patched = owners[owner](wb, self._seed(), "revise")
                    self._merge_and_log(wb, patched, owner+"(revise)")
                    touched.add(owner)

        final = wb.read()
        missing = [k for k in REQUIRED_KEYS if not non_empty(final.get(k))]
        if missing:
            wb.log("Orchestrator", f"æœ€ç»ˆè¡¥é½ï¼šç¼ºå¤± {missing}")
            for k in missing:
                owner = next((owner for owner,keys in RESP_FIELDS.items() if k in keys), None)
                if owner == "å­¦ç±ä¸å‘å±•é˜¶æ®µ":
                    self._merge_and_log(wb, agent_scholar(wb, self._seed(), "revise"), "å­¦ç±ä¸å‘å±•é˜¶æ®µ(revise-final)")
                elif owner == "å­¦ä¸šç”»åƒ":
                    self._merge_and_log(wb, agent_academic(wb, self._seed(), "revise"), "å­¦ä¸šç”»åƒ(revise-final)")
                elif owner == "äººæ ¼ä¸ä»·å€¼è§‚":
                    self._merge_and_log(wb, agent_values(wb, self._seed(), "revise"), "äººæ ¼ä¸ä»·å€¼è§‚(revise-final)")
                elif owner == "ç¤¾äº¤ä¸åˆ›é€ åŠ›":
                    self._merge_and_log(wb, agent_social_creative(wb, self._seed(), "revise"), "ç¤¾äº¤ä¸åˆ›é€ åŠ›(revise-final)")
                elif owner == "èº«å¿ƒå¥åº·":
                    self._merge_and_log(wb, agent_health(wb, self._seed(), "revise"), "èº«å¿ƒå¥åº·(revise-final)")
            final = wb.read()

        for k in REQUIRED_KEYS:
            if not non_empty(final.get(k)):
                raise RuntimeError(f"å­—æ®µä»ä¸ºç©ºï¼š{k}")

        lvl = final.get("å­¦æœ¯æ°´å¹³", "")
        if lvl not in STRICT_ALLOWED_STRINGS:
            raise RuntimeError("å­¦æœ¯æ°´å¹³ä¸ç¬¦åˆä¸¥æ ¼å››é€‰ä¸€æ ‡å‡†ï¼Œè¯·é‡è¯•ã€‚")

        # æ³¨æ„ï¼šåœ¨è½ç›˜å‰ä¿ç•™ _é‡‡æ ·çº¦æŸ ç”¨äºåœ¨çº¿è¿‡æ»¤çš„é”šå‚è€ƒã€‚è½ç›˜æ—¶ä½ ä¹Ÿå¯ä»¥é€‰æ‹© pop æ‰ã€‚
        return final, wb.discussion

# ================== QuotaSchedulerï¼šæŒ‰æ¯”ä¾‹ç”Ÿæˆâ€œç›®æ ‡å­¦æœ¯æ°´å¹³â€ ==================
def _default_quota(n_total: int) -> List[Dict[str,Any]]:
    slots = []
    triplets = [(g,s,c) for g in GRADES for s in GENDERS for c in SUBJ_CLUSTERS.keys()]
    for i in range(n_total):
        g, s, c = triplets[i % len(triplets)]
        slots.append({"å¹´çº§": g, "æ€§åˆ«": s, "ä¼˜åŠ¿å­¦ç§‘åå‘": SUBJ_CLUSTERS[c]})
    random.shuffle(slots)
    return slots

def _cycle_levels_by_mix(n_total: int, mix: Dict[str, float]) -> List[str]:
    import math, random
    targets = []
    alloc = {k: int(round(mix.get(k,0.0) * n_total)) for k in STRICT_ALLOWED_STRINGS}
    diff = n_total - sum(alloc.values())
    if diff != 0:
        order = sorted(STRICT_ALLOWED_STRINGS, key=lambda k: mix.get(k,0.0), reverse=True)
        i = 0
        while diff != 0:
            k = order[i % len(order)]
            if diff > 0:
                alloc[k] += 1; diff -= 1
            else:
                if alloc[k] > 0:
                    alloc[k] -= 1; diff += 1
            i += 1
    for k, c in alloc.items():
        targets.extend([k] * max(0, c))
    random.shuffle(targets)
    if len(targets) < n_total:
        pad = list(STRICT_ALLOWED_STRINGS)
        while len(targets) < n_total:
            targets.append(random.choice(pad))
    return targets[:n_total]

class QuotaScheduler:
    def __init__(self, n_total: int, user_quota_json: Optional[str] = None, level_mix: Optional[Dict[str,float]] = None):
        if user_quota_json:
            try:
                arr = json.loads(user_quota_json)
                assert isinstance(arr, list) and all(isinstance(x, dict) for x in arr)
                self.slots = arr
            except Exception:
                self.slots = _default_quota(n_total)
        else:
            self.slots = _default_quota(n_total)
        mix = level_mix or {LEVELS[0]:0.25, LEVELS[1]:0.25, LEVELS[2]:0.25, LEVELS[3]:0.25}
        targets = _cycle_levels_by_mix(n_total, mix)
        for i, t in enumerate(targets):
            self.slots[i]["ç›®æ ‡å­¦æœ¯æ°´å¹³"] = t
        self.idx = 0
        self.total = n_total
    def has_next(self) -> bool:
        return self.idx < self.total
    def next_slot(self) -> Dict[str,Any]:
        if not self.has_next(): return {}
        slot = self.slots[self.idx]; self.idx += 1; return slot

# ================== æœ¬åœ°è½ç›˜ï¼ˆè‡ªåŠ¨ JSONLï¼‰ ==================
def _ensure_dirs():
    base = os.path.join(os.getcwd(), "output")
    if not os.path.exists(base): os.makedirs(base, exist_ok=True)
    return base

def _init_run_dir():
    base = _ensure_dirs()
    run_id = f"run_{int(time.time())}"
    run_dir = os.path.join(base, run_id)
    os.makedirs(run_dir, exist_ok=True)
    return run_id, run_dir

def _chunk_path(run_dir: str, chunk_no: int) -> str:
    return os.path.join(run_dir, f"students_chunk_{chunk_no}.jsonl")

def _append_record(run_dir: str, chunk_no: int, record: Dict[str, Any]):
    path = _chunk_path(run_dir, chunk_no)
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")

def _append_failure(run_dir: str, failure: Dict[str, Any]):
    path = os.path.join(run_dir, "failures.jsonl")
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(failure, ensure_ascii=False) + "\n")

def _count_lines(path: str) -> int:
    if not os.path.exists(path): return 0
    cnt = 0
    with open(path, "r", encoding="utf-8") as f:
        for _ in f: cnt += 1
    return cnt

def _recover_progress_from_disk(run_dir: str, chunk_size: int) -> Tuple[int,int,int]:
    files = sorted(glob.glob(os.path.join(run_dir, "students_chunk_*.jsonl")))
    if not files: return 1, 0, 1
    def _num(p):
        m = re.search(r"students_chunk_(\d+)\.jsonl$", p)
        return int(m.group(1)) if m else 0
    files.sort(key=_num)
    last = files[-1]; last_no = _num(last)
    done_in_last = _count_lines(last)
    chunk_idx = last_no; in_chunk_idx = done_in_last
    global_idx = (chunk_idx-1)*chunk_size + in_chunk_idx + 1
    if in_chunk_idx >= chunk_size: chunk_idx += 1; in_chunk_idx = 0
    return chunk_idx, in_chunk_idx, global_idx

def _load_chunk_preview(run_dir: str, chunk_idx: int, max_items: int = 6) -> List[Dict[str, Any]]:
    path = _chunk_path(run_dir, chunk_idx)
    if not os.path.exists(path): return []
    lines = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line: continue
            try: lines.append(json.loads(line))
            except: pass
    return lines[-max_items:]

# ================== UI ==================
st.set_page_config(page_title="å¤šæ™ºèƒ½ä½“ç”»åƒç”Ÿæˆï¼ˆå‰ç½®æ§åˆ¶+äº¤äº’æ§åˆ¶å°+åˆ†å¸ƒé”šå®šï¼‰", page_icon="ğŸ§©", layout="wide")
st.title("ğŸ§© å­¦ç”Ÿç”»åƒ Â· å¤šæ™ºèƒ½ä½“å®æ—¶åä½œï¼ˆå‰ç½®æ§åˆ¶ + äº¤äº’æ§åˆ¶å° + åˆ†å¸ƒé”šå®šï¼‰")

with st.sidebar:
    st.subheader("åœ¨çº¿å‰ç½®æ§åˆ¶")
    simhash_th = st.number_input("ç›¸ä¼¼åº¦é˜ˆï¼ˆSimHashæ±‰æ˜è·ç¦»ï¼Œâ‰¤è§†ä¸ºè¿‡è¿‘éœ€é‡ç”Ÿï¼‰", 0, 16, SIMHASH_HAMMING_THRESHOLD_DEFAULT)
    user_quota_json = st.text_area("è‡ªå®šä¹‰é…é¢JSONï¼ˆå¯é€‰ï¼‰", placeholder='[{"å¹´çº§":"åˆä¸€","æ€§åˆ«":"å¥³","ä¼˜åŠ¿å­¦ç§‘åå‘":["è‹±è¯­","ç”Ÿç‰©"]}]')
    show_console = st.toggle("æ˜¾ç¤ºäº¤äº’æ§åˆ¶å°ï¼ˆPrompt/Output/Issuesï¼‰", value=True)
    # æ–°å¢ï¼šå­¦æœ¯æ°´å¹³æ¯”ä¾‹
    level_mix_text = st.text_input(
        "å­¦æœ¯æ°´å¹³æ¯”ä¾‹ï¼ˆé«˜/ä¸­/ä½/å·®ï¼‰ï¼Œå¦‚ï¼šé«˜:0.25,ä¸­:0.25,ä½:0.25,å·®:0.25",
        value="é«˜:0.25,ä¸­:0.25,ä½:0.25,å·®:0.25"
    )
    st.caption("æ¯”ä¾‹ä½œä¸ºé‡‡æ ·å…ˆéªŒå†™å…¥ç™½æ¿ï¼Œé©±åŠ¨å››æ¡£åˆ†å¸ƒï¼›å¹¶è”åŠ¨ä¸‹æ¸¸ç»´åº¦é¿å…â€˜æ¸…ä¸€è‰²åé«˜â€™ã€‚")

with st.expander("è¯´æ˜", expanded=False):
    st.markdown("""
- **é…é¢åˆ†æ¡¶è°ƒåº¦**ã€**è½»é‡è¿‡æ»¤**ã€**SimHash å»åŒè´¨åŒ–**ï¼›ä¸è¿‡å…³å³é‡é‡‡ï¼›  
- è‡ªåŠ¨è½ç›˜ï¼š`output/<run_id>/students_chunk_{i}.jsonl`ï¼›å¤±è´¥æ ·æœ¬ `failures.jsonl`ï¼›  
- å­¦æœ¯æ°´å¹³å››é€‰ä¸€ï¼ˆå›ºå®šæ–‡æ¡ˆï¼‰ï¼›ä»£ç†åï¼šå§“ 1â€“2 éŸ³èŠ‚ã€å 1â€“3 éŸ³èŠ‚ï¼Œæ¯èŠ‚â€œæ‹¼éŸ³+1~5 å£°è°ƒâ€ï¼Œä¸‹åˆ’çº¿åˆ†éš”ã€‚  
- **åˆ†å¸ƒé”šå®š**ï¼šä¾§è¾¹æ æ§åˆ¶â€œé«˜/ä¸­/ä½/å·®â€æ¯”ä¾‹ï¼›`agent_academic` å¼ºåˆ¶è¾“å‡ºã€`Validator` å…œåº•ä¸€è‡´ï¼›  
- **ä¹è§‚åç½®æŠ‘åˆ¶**ï¼šä»·å€¼è§‚/åˆ›é€ åŠ›/å¿ƒç†å¥åº·éšé”šè‡ªé€‚åº”ï¼Œè½»é‡è¿‡æ»¤ä¸­è¦æ±‚â€œä¸­/è¾ƒä½/ä½â€çš„**æœ€å°è®¡æ•°**ï¼ˆç›®æ ‡ä¸ºâ€œä¸­/ä½/å·®â€æ—¶ç”Ÿæ•ˆï¼‰ã€‚  
""")

left, right = st.columns([1,3])
with left:
    n = st.number_input("ç”Ÿæˆæ•°é‡ï¼ˆæ— é™åˆ¶ï¼‰", min_value=1, value=100, step=1)
    rounds = st.number_input("æœ€å¤§åå•†è½®æ•°ï¼ˆæ— é™åˆ¶ï¼‰", min_value=1, value=3, step=1)
    chunk_size = CHUNK_SIZE_DEFAULT
    start_btn = st.button("å¼€å§‹ç”Ÿæˆ", type="primary")
    pause_btn = st.button("æš‚åœç”Ÿæˆ â¸")
    resume_btn = st.button("ç»§ç»­ç”Ÿæˆ â–¶ï¸")

# ----------- çŠ¶æ€åˆå§‹åŒ– -----------
if "running" not in st.session_state: st.session_state.running = False
if "paused" not in st.session_state: st.session_state.paused = False
if "total_n" not in st.session_state: st.session_state.total_n = 0
if "max_rounds" not in st.session_state: st.session_state.max_rounds = 3
if "chunk_size" not in st.session_state: st.session_state.chunk_size = CHUNK_SIZE_DEFAULT
if "chunks_total" not in st.session_state: st.session_state.chunks_total = 0
if "chunk_idx" not in st.session_state: st.session_state.chunk_idx = 1
if "in_chunk_idx" not in st.session_state: st.session_state.in_chunk_idx = 0
if "global_idx" not in st.session_state: st.session_state.global_idx = 1
if "orch" not in st.session_state: st.session_state.orch = None
if "run_id" not in st.session_state: st.session_state.run_id = None
if "run_dir" not in st.session_state: st.session_state.run_dir = None
if "last_item" not in st.session_state: st.session_state.last_item = None
if "last_dialog" not in st.session_state: st.session_state.last_dialog = []
if "last_error" not in st.session_state: st.session_state.last_error = None
if "quota" not in st.session_state: st.session_state.quota = None
if "sim_gate" not in st.session_state: st.session_state.sim_gate = SimilarityGate(threshold=simhash_th)
if "level_mix" not in st.session_state: st.session_state.level_mix = _parse_level_mix(level_mix_text)

# ----------- æ§åˆ¶æŒ‰é’® -----------
if start_btn:
    st.session_state.running = True
    st.session_state.paused = False
    st.session_state.total_n = int(n)
    st.session_state.max_rounds = int(rounds)
    st.session_state.chunk_size = int(chunk_size)
    st.session_state.chunks_total = math.ceil(st.session_state.total_n / st.session_state.chunk_size)
    st.session_state.run_id, st.session_state.run_dir = _init_run_dir()
    st.session_state.chunk_idx = 1
    st.session_state.in_chunk_idx = 0
    st.session_state.global_idx = 1
    st.session_state.orch = Orchestrator(max_rounds=st.session_state.max_rounds)
    st.session_state.last_item = None
    st.session_state.last_dialog = []
    st.session_state.last_error = None
    st.session_state.level_mix = _parse_level_mix(level_mix_text)
    st.session_state.quota = QuotaScheduler(
        st.session_state.total_n,
        user_quota_json=user_quota_json,
        level_mix=st.session_state.level_mix
    )
    st.session_state.sim_gate = SimilarityGate(threshold=simhash_th)
    st.success(f"è¾“å‡ºç›®å½•ï¼š{st.session_state.run_dir}")
    _st_rerun()

if pause_btn and st.session_state.running:
    st.session_state.paused = True

if resume_btn and st.session_state.running:
    st.session_state.paused = False
    if st.session_state.run_dir:
        ck_idx, in_ck_idx, g_idx = _recover_progress_from_disk(st.session_state.run_dir, st.session_state.chunk_size)
        st.session_state.chunk_idx = ck_idx
        st.session_state.in_chunk_idx = in_ck_idx
        st.session_state.global_idx = g_idx
    _st_rerun()

# ----------- è¿›åº¦æ¡å®¹å™¨ -----------
chunk_prog_box = st.empty()
prog = st.empty()
status = st.empty()
preview_live = st.container()
console = st.container()
cards = st.container()

# ----------- ä¸»å¾ªç¯ -----------
if st.session_state.running:
    chunk_prog = (st.session_state.chunk_idx-1) / max(1, st.session_state.chunks_total)
    chunk_prog_box.progress(
        min(chunk_prog, 1.0),
        text=f"åˆ†ç‰‡è¿›åº¦ï¼šç¬¬ {st.session_state.chunk_idx}/{st.session_state.chunks_total} ç‰‡ï¼ˆæ¯ç‰‡ {st.session_state.chunk_size} æ¡ï¼‰ Â· è¾“å‡ºç›®å½•ï¼š{st.session_state.run_dir or 'ï¼ˆæœªåˆå§‹åŒ–ï¼‰'}"
    )
    if st.session_state.paused:
        status.warning(f"å·²æš‚åœï¼ˆå½“å‰ç‰‡å·²å®Œæˆ {st.session_state.in_chunk_idx}/{st.session_state.chunk_size} æ¡ï¼›ç»§ç»­åè‡ªåŠ¨ç»­å†™ï¼‰")
    else:
        current_chunk_total = min(st.session_state.chunk_size, st.session_state.total_n - (st.session_state.chunk_idx-1)*st.session_state.chunk_size)
        status.info(f"ç”Ÿæˆä¸­ï¼šå…¨å±€ç¬¬ {st.session_state.global_idx}/{st.session_state.total_n} æ¡ Â· å½“å‰ç‰‡ç¬¬ {st.session_state.in_chunk_idx+1}/{current_chunk_total} æ¡")

    current_chunk_total = min(st.session_state.chunk_size, st.session_state.total_n - (st.session_state.chunk_idx-1)*st.session_state.chunk_size)
    prog.progress(st.session_state.in_chunk_idx / max(1, current_chunk_total), text=f"å½“å‰ç‰‡è¿›åº¦ï¼š{st.session_state.in_chunk_idx}/{current_chunk_total}")

    # å³æ—¶é¢„è§ˆï¼ˆæœ¬æ¡ï¼‰
    with preview_live:
        st.subheader("ğŸ–¥ï¸ å³æ—¶é¢„è§ˆï¼ˆæœ¬æ¡ç”Ÿæˆçš„ç”»åƒï¼‰")
        if st.session_state.last_error: st.error(st.session_state.last_error)
        if st.session_state.last_item:
            with st.expander(f"{st.session_state.last_item.get('å§“å')} â€” {st.session_state.last_item.get('å¹´çº§')} Â· ä»£ç†åï¼š{st.session_state.last_item.get('ä»£ç†å')}", expanded=True):
                st.json(st.session_state.last_item, expanded=False)

    # äº¤äº’æ§åˆ¶å°ï¼ˆæœ¬æ¡ï¼‰
    if show_console and st.session_state.last_dialog:
        with console:
            st.subheader("ğŸ–§ äº¤äº’æ§åˆ¶å°ï¼ˆæœ¬æ¡ï¼‰")
            tabs = st.tabs(["å­¦ç±ä¸å‘å±•é˜¶æ®µ", "å­¦ä¸šç”»åƒ", "äººæ ¼ä¸ä»·å€¼è§‚", "ç¤¾äº¤ä¸åˆ›é€ åŠ›", "èº«å¿ƒå¥åº·", "Validator", "Whiteboard RAW"])

            def _show_logs(agent_key: str):
                logs = [m for m in st.session_state.last_dialog if m["speaker"].startswith(agent_key)]
                if not logs:
                    st.info("æš‚æ— æ—¥å¿—")
                else:
                    for m in logs[-12:]:
                        st.markdown(f"**{m['speaker']}**")
                        st.code(m["content"])

            with tabs[0]:
                _show_logs("å­¦ç±ä¸å‘å±•é˜¶æ®µ")
            with tabs[1]:
                _show_logs("å­¦ä¸šç”»åƒ")
            with tabs[2]:
                _show_logs("äººæ ¼ä¸ä»·å€¼è§‚")
            with tabs[3]:
                _show_logs("ç¤¾äº¤ä¸åˆ›é€ åŠ›")
            with tabs[4]:
                _show_logs("èº«å¿ƒå¥åº·")
            with tabs[5]:
                _show_logs("Validator")
                import pandas as pd
                issues_rows = []
                for m in reversed(st.session_state.last_dialog):
                    if m["speaker"] == "Validator(issues)":
                        try:
                            arr = json.loads(m["content"])
                            if isinstance(arr, list):
                                issues_rows = arr; break
                        except: pass
                if issues_rows:
                    df = pd.DataFrame(issues_rows)
                    st.dataframe(df, use_container_width=True)
                else:
                    st.info("æœªæ•è·åˆ°ç»“æ„åŒ– issuesã€‚")
            with tabs[6]:
                for m in st.session_state.last_dialog[-40:]:
                    st.markdown(f"**{m['speaker']}**")
                    st.code(m["content"])

    # ç‰‡å°¾é¢„è§ˆ
    with cards:
        st.subheader("ğŸ“‚ å½“å‰ç‰‡æœ«å°¾é¢„è§ˆï¼ˆæ¥è‡ªæœ¬åœ°æ–‡ä»¶ï¼‰")
        if st.session_state.run_dir:
            preview_items = _load_chunk_preview(st.session_state.run_dir, st.session_state.chunk_idx, max_items=6)
            if preview_items:
                start_idx = max(1, st.session_state.in_chunk_idx - len(preview_items) + 1)
                for idx, item in enumerate(preview_items, start=start_idx):
                    with st.expander(f"#{idx} â€” {item.get('å§“å')}ï¼ˆ{item.get('å¹´çº§')}ï¼‰ Â· ä»£ç†åï¼š{item.get('ä»£ç†å')}", expanded=False):
                        st.json(item, expanded=False)
            else:
                st.info("å½“å‰ç‰‡æš‚æ— å·²å†™å…¥è®°å½•ã€‚")

    # ç»“æŸ
    if st.session_state.global_idx > st.session_state.total_n or (st.session_state.quota and not st.session_state.quota.has_next()):
        prog.progress(1.0, text="å½“å‰ç‰‡è¿›åº¦ï¼šå®Œæˆ âœ…")
        chunk_prog_box.progress(1.0, text="åˆ†ç‰‡è¿›åº¦ï¼šå…¨éƒ¨å®Œæˆ âœ…")
        status.success(f"å…¨éƒ¨ç”Ÿæˆå®Œæˆï¼æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{st.session_state.run_dir}")
        st.session_state.running = False

    # æ¨è¿›ä¸€ä¸ªæ§½ä½ï¼ˆæœªæš‚åœæ—¶ï¼‰
    elif not st.session_state.paused:
        slot = st.session_state.quota.next_slot() if st.session_state.quota else {}
        target_sid = st.session_state.global_idx
        orch: Orchestrator = st.session_state.orch
        accepted = False
        error_trace = None

        for attempt in range(1, MAX_RETRIES_PER_SLOT+1):
            try:
                item, dialog = orch.run_one(target_sid, sampling_hint=slot)

                # è½»é‡è¿‡æ»¤ï¼ˆå«ä¹è§‚åç½®æŠ‘åˆ¶ï¼‰
                ok, reasons = _light_filter(item)
                if not ok:
                    error_trace = f"è½»é‡è¿‡æ»¤ä¸é€šè¿‡ï¼š{'; '.join(reasons)}"
                    raise RuntimeError(error_trace)

                # è‡ªç›¸ä¼¼åº¦é—¨æ§
                key_text = "ï½œ".join([
                    str(item.get("å¹´çº§","")), str(item.get("æ€§åˆ«","")),
                    " ".join(item.get("äººæ ¼",[]) if isinstance(item.get("äººæ ¼"), list) else [str(item.get("äººæ ¼",""))]),
                    str(item.get("ä»·å€¼è§‚","")), str(item.get("ç¤¾äº¤å…³ç³»","")),
                    str(item.get("åˆ›é€ åŠ›","")), str(item.get("å¿ƒç†å¥åº·",""))
                ])
                if st.session_state.sim_gate.too_similar(key_text):
                    error_trace = f"ä¸å·²æœ‰æ ·æœ¬è¿‡è¿‘ï¼ˆSimHashâ‰¤{st.session_state.sim_gate.threshold}ï¼‰"
                    raise RuntimeError(error_trace)

                # é€šè¿‡ï¼šå†™ç›˜ã€ç™»è®°ç›¸ä¼¼åº¦æ± ã€ç¼“å­˜ç•Œé¢
                _append_record(st.session_state.run_dir, st.session_state.chunk_idx, item)
                st.session_state.sim_gate.accept(key_text)

                st.session_state.in_chunk_idx += 1
                st.session_state.global_idx += 1
                st.session_state.last_item = item
                st.session_state.last_dialog = dialog
                st.session_state.last_error = None

                if st.session_state.in_chunk_idx >= st.session_state.chunk_size:
                    st.session_state.in_chunk_idx = 0
                    st.session_state.chunk_idx += 1

                accepted = True
                break

            except Exception as e:
                err_msg = f"æ§½ä½{slot} Â· å°è¯•{attempt}/{MAX_RETRIES_PER_SLOT}å¤±è´¥ï¼š{e}"
                st.session_state.last_error = err_msg
                if attempt == MAX_RETRIES_PER_SLOT:
                    _append_failure(st.session_state.run_dir, {"slot": slot, "sid": target_sid, "error": str(e)})
                    st.session_state.global_idx += 1

        if not accepted and error_trace:
            st.error(error_trace)

        _st_rerun()
